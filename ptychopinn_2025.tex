%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc,positioning,fit,backgrounds,matrix,arrows.meta}
\usepackage{ifthen}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{adjustbox}
\sisetup{
    round-mode = places,
    round-precision = 2,
    detect-all
}
\usepackage{multirow}
\definecolor{bestval}{RGB}{0,100,0}

%% Theorem styles
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

% Color definitions for the figure
\definecolor{traincol}{RGB}{0,128,128}  % teal for TRAIN
\definecolor{testcol}{RGB}{108,0,158}   % purple for TEST
\definecolor{pinncol}{RGB}{0,95,184}    % blue for PINN row label
\definecolor{basecol}{RGB}{194,79,0}    % orange for Baseline row label
\definecolor{hdrbg}{RGB}{245,245,245}   % header ribbon bg

% Author and affiliation macros for article class
\newcommand{\fnm}[1]{#1}
\newcommand{\sur}[1]{#1}
\newcommand{\affil}[2][]{#2}

\raggedbottom

\begin{document}

\title{Towards single-shot coherent imaging via overlap-free ptychography}

% TODO: uncomment and finalize author block before submission
%\author{Oliver Hoidn$^1$,  \\
%$^1$SLAC National Accelerator Laboratory, Menlo Park, California, USA \\
%\texttt{ohoidn@slac.stanford.edu}}

\date{}

\maketitle

\begin{abstract}
Single-shot coherent imaging is important for XFEL science because it lowers photon dose, removes the overhead of overlapping scans, and enables real-time feedback. We extend PtychoPINN~\cite{Hoidn2023PtychoPINN}, a physics-constrained neural network for self-supervised ptychographic reconstruction, to deliver \emph{overlap-free, single-shot} reconstructions in a Fresnel (near-field) CDI geometry and also accelerate conventional multi-shot ptychography. PtychoPINN is self-supervised: it learns directly from raw diffraction patterns by enforcing a differentiable forward model of coherent scattering together with a Poisson photon-counting likelihood and a calibrated intensity scale. The framework enforces optional real-space constraints through coordinate-based grouping, which enables multi-shot reconstruction under arbitrary scan geometries. The network reconstructs approximately $6.1\times10^3$ diffraction patterns/s at $64\times64$ pixel resolution, remains accurate at low counts ($\sim\!10^4$ photons/frame) and zero real-space overlap, and achieves high fidelity with an order of magnitude less training data than a supervised baseline with the same neural network architecture. It also shows qualitative cross-facility transfer without retraining. In the Fresnel regime we obtain \emph{single-shot} reconstructions. This unifies single-exposure Fresnel CDI and overlapped ptychography within one computational framework, enabling high-resolution, dose-efficient operando imaging at XFELs at substantially higher throughput than previously feasible.

\end{abstract}

\section{Introduction}\label{sec1}
Modern light sources, such as fourth-generation synchrotrons and X-ray Free-Electron Lasers (XFELs), generate coherent diffraction data far faster than images can be reconstructed~\cite{LCLSIIHE_DesignPerf}. This growing gap between acquisition and analysis precludes real-time feedback and on-the-fly experimental steering, both essential for maximizing the scientific output of these facilities.

Ptychographic CDI is a cornerstone x-ray nanoscale imaging technique~\cite{GuizarSicairos2021PhysicsToday}, but it faces two compounding bottlenecks. First, classical iterative algorithms like the Ptychographic Iterative Engine (PIE) require $\sim$60--70\% scan overlap for robust convergence and process only $\sim$0.1--1 diffraction patterns per second on standard hardware~\cite{Bunk2008Overlap,Maiden2009UltramicroscopyPIE}; even GPU-accelerated solvers struggle to keep pace with high-repetition-rate sources~\cite{Marchesini2016SHARP,Babu2023EdgePtycho}. Second, supervised ML approaches~\cite{Cherukara2020PtychoNN} can accelerate inference but are limited by poor generalization and the need for large labeled training sets produced by iterative solvers that themselves require overlap. Moreover, single-frame supervised methods cannot exploit overlap redundancy when it is available, limiting reconstruction resolution. Neither approach unifies speed with an optimal handling of overlap constraints.

A differentiable forward model of multiple coherent scattering shots, when paired with a self-supervised training objective, resolves both limitations at once. Because the network learns directly from raw diffraction patterns via a Poisson photon-counting likelihood~\cite{Thibault2012NJPML,Seifert2023PoissonGaussian}, no labeled reconstructions are needed, removing the data-generation bottleneck that limits supervised methods. At the same time, the degree of real-space overlap enters the framework as an explicit, tunable parameter rather than a hard requirement: when the illuminating probe provides sufficient phase diversity, the diffraction-domain likelihood alone anchors the reconstruction, and overlap can be reduced to zero. In particular, a curved or defocused probe imprints spatially varying phase onto the exit wave, providing the diversity that makes single-shot phase retrieval well-posed. This is the principle underlying Fresnel CDI~\cite{Williams2006FresnelCDI,Stockmar2013Nearfield}. We use ``single-shot'' throughout in this strict \emph{Fresnel} sense: a single diffraction measurement with a structured probe, without lateral scanning, beam multiplexing~\cite{Sidorenko2015Optica,Kharitonov2022SciRep}, or downstream modulators~\cite{Zhang2016CMI,Dong2018CMI}.

Our previous work~\cite{Hoidn2023PtychoPINN} demonstrated this physics-constrained approach on synthetic data; here, we extend PtychoPINN to realistic, extended probes and arbitrary scan geometries. We evaluate the model under both typical and non-ideal conditions, including low photon dose and large position jitter, and demonstrate good performance on experimental data from the Advanced Photon Source (APS) and the Linac Coherent Light Source (LCLS). Specifically, this work demonstrates:
\begin{enumerate}
  \item self-supervised reconstruction of experimental data (APS, LCLS) at ${\sim}6.1\times10^3$ diffraction patterns/s;
  \item overlap-free, single-shot reconstruction in Fresnel CDI geometry;
  \item dose-efficient imaging via Poisson likelihood at ${\sim}10^4$ photons/frame;
  \item an order-of-magnitude improvement in data efficiency over a supervised baseline with the same network architecture.
\end{enumerate}
In this study, APS and LCLS reconstructions are performed in overlap-free single-shot mode, except in explicitly labeled overlap ablations.


\begin{figure}[t]
  \centering
  % Row 1: Idealized
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/idealized_cdi_scaled_v5_small.png}
    \caption{Idealized — CDI}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/idealized_ptycho_scaled_v5_small.png}
    \caption{Idealized — Ptycho}
  \end{subfigure}

  \vspace{0.6em}

  % Row 2: Semi-synthetic
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hybrid_cdi_scaled_v5_small.png}
    \caption{Semi-synthetic — CDI}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hybrid_ptycho_scaled_v5_small.png}
    \caption{Semi-synthetic — Ptycho}
  \end{subfigure}
  \caption{Reconstruction comparison across probe types and acquisition modes. Rows: idealized probe (Gaussian-smoothed disk, uniform phase) vs semi-synthetic (experimental probe, synthetic object). Columns: single-shot CDI vs overlapped ptychography.}
  \label{fig:recon_2x2}
\end{figure}

\section{Methods and Architecture}
\label{sec:methods}

Coherent diffractive imaging requires the recovery of a complex object from intensity-only diffraction measurements. Reconstruction methods enforce two complementary constraints: a reciprocal-space constraint requiring predicted intensities to match data (via a physics-based forward model), and a real-space constraint enforcing consistency between overlapping views. In our framework, the reciprocal-space constraint is enforced directly via a differentiable forward model and a Poisson likelihood. Real-space overlap is handled via a translation-aware merging operator. Crucially, this allows overlap to be treated as a flexible experimental parameter rather than a hard requirement; setting the group size to a single frame ($C_g = 1$) removes overlap constraints entirely, enabling ``single-shot'' reconstruction when the probe provides sufficient phase diversity.

We train PtychoPINN to perform self-supervised ptychographic reconstruction by composing a learned inverse map with a differentiable forward model of coherent scattering. This section consolidates the formulation, operators, normalization, network design (including extended-probe handling), and training objective.

\subsection{Formulation and Forward Model}
\label{sec:formulation}

We learn an inverse map $G: X \!\to\! Y$ from diffraction space to real space and optimize it by composing with a differentiable forward model $F: Y \!\to\! X$. The overall autoencoder is $F \circ G$, trained to match measured diffraction statistics without ground-truth images.

\paragraph{Data model and notation.}
Each training sample comprises $C_g$ diffraction amplitude images $\{x_k\}_{k=1}^{C_g}$ acquired at probe coordinates $\{\vec{r}_k\}_{k=1}^{C_g}$. The network $G(x,r)$ outputs $C_g$ complex object patches $\{O_k\}_{k=1}^{C_g}$ on an $N\times N$ grid.  We use:
\begin{itemize}\itemsep3pt
  \item $\mathcal{T}_{\Delta \vec{r}}[\cdot]$: real-space translation by $\Delta \vec{r}$,
  \item $\mathrm{Pad}[\cdot]$: zero-padding to a canvas large enough to contain all translated patches,
  \item $\mathrm{Pad}_{N/4}[\cdot]$: zero-padding that embeds a central $N/2\times N/2$ tile into an $N\times N$ grid,
  \item $\mathrm{Crop}_N[\cdot]$: center-cropping to $N\times N$,
  \item $\mathbf{1}$: an all-ones array of appropriate size,
  \item $\odot$: elementwise (Hadamard) product.
\end{itemize}

\paragraph{Constraint map ($F_c$): translation-aware merging.}
To enforce overlap consistency, per-patch reconstructions are merged in a translation-aligned frame:
\begin{align}
  O_{\text{region}}(\vec{r})
  \;=\;
  \frac{
    \sum_{k=1}^{C_g}\; \mathcal{T}_{-\vec{r}_k}\!\left[\mathrm{Pad}\!\left(O_k\right)\right]
  }{
    \sum_{k=1}^{C_g}\; \mathcal{T}_{-\vec{r}_k}\!\left[\mathrm{Pad}\!\left(\mathbf{1}\right)\right] + \epsilon
  },
  \qquad \epsilon=10^{-3}.
  \label{eq:constraintmap}
\end{align}
This "translational pooling'' applies to arbitrary scan geometries.

\paragraph{Coordinate-aware grouping.}
Training groups are formed locally by nearest-neighbor sampling. For each anchor $\vec{r}_i$, let $\mathcal{N}_K(\vec{r}_i)$ be its $K$ nearest distinct neighbors. A group $\mathcal{G}_{i,j}$ draws $C_g-1$ neighbors uniformly without replacement:
\[
\mathcal{G}_{i,j}=\{\vec{r}_i\}\cup S_{i,j},\quad S_{i,j}\subset \mathcal{N}_K(\vec{r}_i),\; |S_{i,j}|=C_g-1,
\]
repeated $n_{\text{samples}}$ times per anchor. If duplicate neighbor sets are disallowed, the effective number of distinct groups per anchor is
\[
n_{\text{eff}} \;=\; \min\!\left(n_{\text{samples}},\, \binom{K}{C_g - 1}\right),
\]
so the total number of training examples is $N_{\text{scan}} \times n_{\text{eff}}$, with the combinatorial upper bound $N_{\text{scan}} \binom{K}{C_g - 1}$. Choosing $n_{\text{samples}} > 1$ augments the dataset through combinatorial re-grouping while preserving local spatial consistency.

Coordinates within each group are expressed in a stable local frame by re-centering to the group centroid
\[
\vec{r}_{\text{global}}=\frac{1}{C_g}\sum_{k=1}^{C_g}\vec{r}_k,\qquad
\vec{r}^{\,\text{rel}}_k=\vec{r}_k-\vec{r}_{\text{global}}.
\]

\paragraph{Diffraction map ($F_d$): coherent scattering.}
Given $O_{\text{region}}$, the $k$th translated object patch and exit wave are
\begin{align}
  O'_k(\vec{r}) &= \mathrm{Crop}_N\!\left[\mathcal{T}_{\vec{r}^{\,\text{rel}}_k}\!\left(O_{\text{region}}\right)\right], \\
  \Psi_k &= \mathcal{F}\!\left\{ O'_k(\vec{r}) \cdot P(\vec{r}) \right\},
\end{align}
where $P(\vec{r})$ is the (estimated) probe and $\mathcal{F}$ is the 2D Fourier transform. Predicted detector-plane amplitudes include a global intensity scale $e^{\alpha_{\log}}$ that links normalized network outputs to physical photon counts:
\begin{align}
  \hat{A}_k \;=\; |\Psi_k|\; e^{\alpha_{\log}}.
\end{align}

\subsection{Data Preprocessing}
\label{sec:preprocess}

A dataset consists of diffraction images from one or more objects measured with a fixed probe illumination $P$. After grouping images into samples of $C_g$ diffraction patterns each (Section~\ref{sec:formulation}), we normalize the raw diffraction amplitudes to ensure numerical stability during training:
\begin{align}
  x_k \;=\; x'_k \cdot \sqrt{\frac{(N/2)^2}{\big\langle\sum_{i,j} |x'_{ij}|^2\big\rangle}},
  \label{eq:norm}
\end{align}
where $x'$ denotes raw measurements and the average is over all images in the dataset. This choice ensures order-unity activations in the neural network: by Parseval's theorem, unit-amplitude real-space objects produce diffraction power of approximately $N^2/4$, so this normalization maps experimental amplitude images to internal activations of order unity.
%\footnote{The factor of 4 arises from the oversampling requirement that the reconstructed object occupies the central $N/2 \times N/2$ region of an $N \times N$ grid.}

Additionally, we introduce a trainable scalar $\alpha_{\log}$ that converts between the dimensionless internal model activations and per-pixel integrated amplitudes. As discussed in Section~\ref{sec:loss}, the role of $\alpha_{\log}$ is to convert the output \emph{intensity} into physical units of photons per pixel. The final, scaled, network input is $x_{\text{in}} = x \cdot e^{-\alpha_{\log}}$.

\subsection{Neural Network Architecture}
\label{sec:nn}

The inverse map $G$ follows an encoder–decoder design (as in \cite{Hoidn2023PtychoPINN}; see also \cite{Vong2025GeneralizablePtycho} for a PyTorch implementation), conditioned on $\{x_k\}_{k=1}^{C_g}$ and $\{\vec{r}^{\,\text{rel}}_k\}_{k=1}^{C_g}$, and outputs complex patches $\{O_k\}_{k=1}^{C_g}$. To respect oversampling while avoiding truncation artifacts from realistic probes with extended tails, the decoder allocates most capacity to the central, well-posed region and a lightweight continuation to the periphery.

\paragraph{Handling extended probes.}
CNN architectures are limited to modest dimensions ($N \leq 128$) because convolutional receptive fields capture long-range interactions only inefficiently in this Fourier inversion setting, and we must furthermore restrict high-resolution reconstruction to the central $N/2 \times N/2$ region to satisfy oversampling conditions \cite{miao1999extending}. Probes with extended tails force inefficient use of this limited number of pixels because the real-space area brightly illuminated by the probe is small compared to the total probe area that must be represented to avoid truncation artifacts from non-zero amplitude at the edge of the real-space grid.

Consequently, given the modest magnitude of $N$, fully inscribing the probe—tails included—within the central $N/2 \times N/2$ pixels may require too much binning. This causes a dilemma: one must choose between truncation artifacts (and possible lack of convergence due to the associated physical inconsistency) and violation of the diffraction-space oversampling condition for coherent imaging.

We resolve this by reconstructing the object in high resolution in the central $N/2 \times N/2$ region of the real-space grid and low resolution in the periphery. Presuming the absence of high spatial frequency components in the probe tail, extending the probe times object reconstruction into the periphery does not compromise well-posedness of the inverse problem.

Concretely, we use most channels ($C-4$) of the penultimate decoder layer for the central region and the remaining 4 channels to coarsely reconstruct the periphery:
\begin{align}
  O_{\mathrm{amp}} &= \mathrm{Pad}_{N/4}\!\big(\sigma_A(\mathrm{Conv}(H^{\text{central}}_A))\big)\;+\;
  \sigma_A(\mathrm{ConvUp}(H^{\text{border}}_A))\odot M_{\text{border}},\\
  O_{\mathrm{phase}} &= \mathrm{Pad}_{N/4}\!\big(\pi \tanh(\mathrm{Conv}(H^{\text{central}}_\phi))\big)\;+\;
  \pi \tanh(\mathrm{ConvUp}(H^{\text{border}}_\phi))\odot M_{\text{border}},\\
  O_k &= O_{\mathrm{amp}}\cdot \exp\!\big(i\,O_{\mathrm{phase}}\big),
\end{align}
where $H^{\text{central}}_{\{\cdot\}}$ (the first $C-4$ channels) targets the central region, $H^{\text{border}}_{\{\cdot\}}$ (the last 4 channels) produces a low-resolution continuation, and $M_{\text{border}}$ is a binary mask that isolates the boundary contributions to the outer region. This modification avoids artifacts from truncation of the exit wave and enables stable reconstruction with experimentally realistic probes.

\subsection{Training Objective and Optimization}
\label{sec:loss}

\paragraph{Poisson negative log-likelihood.}
The training procedure optimizes the inverse map $G$ using a negative log-likelihood loss under Poisson statistics:
\begin{align}
  \mathcal{L}_{\text{Poiss}}
  \;=\;
  -\sum_{k,i,j} \log f_{\text{Poiss}}(N_{kij}; \lambda_{kij})
  \;=\;
  \sum_{k,i,j}
  \left(
    \lambda_{kij} - N_{kij}\,\log\lambda_{kij}
  \right),
\end{align}
where $N_{kij} = |x'_{kij}|^2$ is the measured photon count and $\lambda_{kij}=|\hat{A}_{kij}|^2$ is the predicted count.

Since the network operates on normalized inputs (Eq.~\ref{eq:norm}) for numerical stability, a scale parameter $e^{\alpha_{\log}}$ bridges normalized and physical units. When the mean photon flux $N_{\text{photons}}$ is known, we initialize:
\begin{align}
  e^{\alpha_{\log}} \;\leftarrow\; \frac{2\sqrt{N_{\text{photons}}}}{N}.
  \label{eq:alphaloginit}
\end{align}
This ensures predicted intensities match measurement statistics. The parameter $e^{\alpha_{\log}}$ may be fixed or learned (see Table~\ref{tab:config_params}); learning it can absorb modest calibration errors.

\paragraph{Amplitude loss for unknown counts.}
For datasets lacking absolute photon counts, we resort to mean absolute error on normalized amplitudes:
\[
\mathcal{L}_{\text{MAE}}=\sum_{k,i,j} \big|x_{kij} - \hat{A}_{kij}e^{-\alpha_{\log}}\big|.
\]

In the results reported here we do not use any real-space loss; training is driven solely by the diffraction-domain losses (Poisson NLL or MAE, depending on count calibration).

\paragraph{Implementation notes.}
All operators in $F_c$ and $F_d$ are differentiable and implemented with padding-aware translations and FFT-based diffraction. Batching is performed over groups $\mathcal{G}_{i,j}$; nearest-neighbor sampling with $n_{\text{samples}}>1$ provides dataset augmentation while preserving local spatial consistency. Default architectural and training hyperparameters are summarized in Table~\ref{tab:config_params}.

\subsection{Supervised Baseline}
The supervised baseline uses the same encoder-decoder backbone and input representation as PtychoPINN (cf.~PtychoNN~\cite{Cherukara2020PtychoNN}). It is trained with direct image-space supervision on paired diffraction/reference-reconstruction data, without enforcing the differentiable forward model in the training loss. Data splits, normalization, and scan-coordinate conditioning are matched to the PtychoPINN runs so the comparison isolates training paradigm rather than architecture.

\subsection{Datasets and Evaluation Protocol}
We evaluate on APS Velociprobe Siemens-star data, an LCLS (XPP) test pattern dataset (hereafter, LCLS XPP dataset), a synthetic Siemens-star dataset simulated from APS Siemens-star reconstructions (ground truth for Table~\ref{tab:results_16384}), and a synthetic line-pattern dataset of randomly oriented high-aspect-ratio features from~\cite{Hoidn2023PtychoPINN} (used for the overlap ablation in Table~\ref{tab:sim_lines_metrics}). APS and LCLS experiments are run in single-shot mode (one diffraction frame per group), except where overlap ablations are explicitly labeled. For the Siemens-star experiments, we use a spatial holdout: the top half of the scan is used for training and the bottom half for testing. For out-of-distribution transfer, models trained on APS data are evaluated on LCLS data without retraining, with beamline-specific forward parameters (probe/geometry) substituted at inference.


\section{Results}

We report results on APS Velociprobe Siemens-star data, the LCLS XPP dataset, and the synthetic Siemens-star dataset; see Methods for dataset definitions and evaluation protocol.

\subsection{Reconstruction Quality}

Figure~\ref{fig:smalldat} compares reconstructions on APS Siemens-star data across two sampling budgets (512 and 8192 diffraction patterns). PtychoPINN preserves clear edge structure and contrast in both regimes, while the supervised baseline degrades visibly at lower data availability. On the synthetic Siemens-star dataset (simulated from APS Siemens-star reconstructions), PtychoPINN also attains higher phase fidelity than the supervised baseline; see Table~\ref{tab:results_16384}.


\begin{table}[htbp]
\centering
\caption{Reconstruction quality metrics at maximum training set size (16,384 images). Values shown are mean $\pm$ standard deviation across 5 trials. Best values per dataset are highlighted in \textcolor{bestval}{green}.}
\label{tab:results_16384}
\begin{tabular}{@{}llcccc@{}}
\toprule
& & \multicolumn{2}{c}{PSNR (dB)} & \multicolumn{2}{c}{MS-SSIM} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
Dataset & Method & Amplitude & Phase & Amplitude & Phase \\
\midrule
\multirow{2}{*}{synthetic Siemens-star} 
& Supervised baseline &
  $84.83 \pm 0.23$ & 
  $68.62 \pm 0.02$ & 
  $0.930 \pm 0.002$ & 
  $0.912 \pm 0.003$ \\
& PtychoPINN &
  \textcolor{bestval}{$\mathbf{85.53 \pm 0.02}$} & 
  \textcolor{bestval}{$\mathbf{70.54 \pm 0.06}$} & 
  \textcolor{bestval}{$\mathbf{0.955 \pm 0.001}$} & 
  \textcolor{bestval}{$\mathbf{0.962 \pm 0.001}$} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/lowcounts.png}
        \caption{512 diffraction patterns of the Siemens star test pattern.}
        \label{fig:lowcounts}
    \end{subfigure}
    
    \vspace{0.5em}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/8192.png}
        \caption{8192 diffraction patterns of the Siemens star test pattern.}
    \end{subfigure}
    \caption{Comparison of reconstruction quality with different numbers of diffraction patterns.}
    \label{fig:smalldat}
\end{figure}




\subsection{Overlap-Free Reconstruction}

In overlap-free operation, we set the group size to a single diffraction frame ($C_g = 1$), removing overlap-based real-space consistency. Reconstruction then relies entirely on the diffraction likelihood and the known probe structure (defocused probe/Fresnel geometry). Figure~\ref{fig:recon_2x2} illustrates this single-frame mode compared with multi-position ptychography. Quantitative comparisons across overlap and probe-structuring variants on a synthetic line-pattern dataset are summarized in Table~\ref{tab:sim_lines_metrics} (overlap-free $C_g=1$ vs overlap $C_g=4$).
\begin{table}[htbp]
\centering
\caption{Synthetic line-pattern reconstruction metrics on the test split. Ground truth is the simulated object.}
\label{tab:sim_lines_metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
Case & PSNR (dB, Amp) & PSNR (dB, Phase) & SSIM (Amp) & SSIM (Phase) \\
\midrule
overlap-free ($C_g=1$) + idealized probe & 60.67 & 55.25 & 0.620 & 0.410 \\
overlap-free ($C_g=1$) + experimental probe & 68.89 & 63.00 & 0.904 & 0.886 \\
overlap ($C_g=4$) + idealized probe & 71.34 & 86.53 & 0.952 & 0.996 \\
overlap ($C_g=4$) + experimental probe & 73.03 & 64.54 & 0.968 & 0.966 \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Photon-Limited Performance}

Figure~\ref{fig:dose} summarizes resolution (FRC50) as a function of photon dose for Poisson NLL versus MAE. At $\sim10^4$ photons/frame, the Poisson loss preserves high-$q$ detail relative to MAE, consistent with the qualitative comparisons in Fig.~\ref{fig:lowcounts}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/poisson.png}
    \caption{Resolution (FRC50) as a function of on-sample photon dose for two variants of the PtychoPINN framework trained with mean absolute error (MAE) and Poisson negative log likelihood (NLL) reconstruction penalties in the self-supervised loss function}
    \label{fig:dose}
\end{figure}

\subsection{Data Efficiency}

Figure~\ref{fig:ssim} illustrates the reconstruction quality (phase SSIM) as a function of dataset size. PtychoPINN maintains high fidelity (SSIM $> 0.85$) from as few as 1024 diffraction patterns. In contrast, the supervised baseline degrades rapidly below 2048 samples. The horizontal shift between the curves indicates that PtychoPINN achieves comparable quality using roughly an order of magnitude less training data than the supervised baseline. This confirms that enforcing the diffraction forward model acts as a powerful regularizer, reducing the number of samples required to constrain the solution. In this figure, rPIE (Tike) is included as a conventional iterative reference, not as a learned baseline.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ssim (2).png}
    \caption{Structural similarity of PtychoPINN, the supervised baseline, and a conventional iterative reference (rPIE in Tike) as a function of training set size.}
    \label{fig:ssim}
\end{figure}

\subsection{Out-of-distribution Generalization}

Figure~\ref{fig:fivepanel} includes an in-distribution LCLS control (train LCLS XPP, test LCLS XPP) and an out-of-distribution transfer setting (train APS, test LCLS XPP). Under APS$\!\to\!$LCLS shift, the supervised baseline degrades markedly, whereas PtychoPINN preserves edge structure despite phase distortion and reduced quantitative fidelity. The transfer evidence is qualitative but still informative: under the same APS$\!\to\!$LCLS transfer setup, the supervised baseline largely collapses while PtychoPINN retains interpretable structure. The reference column in Fig.~\ref{fig:fivepanel} is an ePIE reconstruction of the LCLS data.

\begin{figure}[t]
  \centering
  \resizebox{0.85\textwidth}{!}{% Scale to 85% of text width
    %\input{figures/outdist_figure.tex}
    % TikZ figure for inclusion in main document
% Required packages should already be loaded by main document

% ---------- GRAPHIC PATH ----------
\graphicspath{{figures/out-dist-fig/}{figures/}{./}}

% ---------- COLORS ----------
\definecolor{traincol}{RGB}{0,128,128}
\definecolor{testcol}{RGB}{108,0,158}
\definecolor{pinncol}{RGB}{0,95,184}
\definecolor{basecol}{RGB}{194,79,0}

% ---------- FILE MACROS ----------
\newcommand{\pinnIDamp}{figures/out-dist-fig/pinn_run1084_amplitude.png}
\newcommand{\pinnIDphs}{figures/out-dist-fig/pinn_run1084_phase.png}
\newcommand{\pinnOODamp}{figures/out-dist-fig/pinn_fly64trained_run1084test_amplitude.png}
\newcommand{\pinnOODphs}{figures/out-dist-fig/pinn_fly64trained_run1084test_phase.png}
\newcommand{\baseIDamp}{figures/out-dist-fig/baseline_run1084_amplitude.png}
\newcommand{\baseIDphs}{figures/out-dist-fig/baseline_run1084_phase.png}
\newcommand{\baseOODamp}{figures/out-dist-fig/baseline_fly64trained_run1084test_amplitude.png}
\newcommand{\baseOODphs}{figures/out-dist-fig/baseline_fly64trained_run1084test_phase.png}
\newcommand{\refAmp}{figures/out-dist-fig/ground_truth_run1084_amplitude.png}
\newcommand{\refPhs}{figures/out-dist-fig/ground_truth_run1084_phase.png}
\newcommand{\probeA}{figures/out-dist-fig/fly64_probe_amplitude.png}   % A: APS-2-ID
\newcommand{\probeB}{figures/out-dist-fig/run1084_probe_amplitude.png} % B: LCLS XPP

% ---------- GLOBAL FONTS ----------
\newcommand{\figfs}{\normalsize}
\newcommand{\scalefs}{\small\bfseries}

% ---------- DIMENSIONS ----------
\newlength{\subw}
\setlength{\subw}{5.0cm}
\newlength{\subh}
\setlength{\subh}{5.0cm}
\newlength{\subsepX}
\setlength{\subsepX}{4mm}
\newlength{\panelsepX}
\setlength{\panelsepX}{16mm}   % wider to keep clear of GT
\newlength{\panelsepY}
\setlength{\panelsepY}{11mm}   % gap between rows

\newlength{\refw}
\setlength{\refw}{5.0cm}
\newlength{\refh}
\setlength{\refh}{5.0cm}
\newlength{\refgap}
\setlength{\refgap}{10mm}

\newlength{\cbarw}
\setlength{\cbarw}{6mm}
\newlength{\cbarh}
\setlength{\cbarh}{5.0cm}
\newlength{\cbarsep}
\setlength{\cbarsep}{4mm}

% ---------- INCLUDE / PLACEHOLDER ----------
\newcommand{\IncludeOrPlaceholderWH}[3]{%
  \IfFileExists{#1}{\includegraphics[width=#2,height=#3,keepaspectratio]{#1}}{%
    \fcolorbox{black!20}{black!5}{\parbox[c][#3][c]{#2}{\centering\footnotesize Missing}}}%
}

% ---------- PANEL FRAMES ----------
\tikzset{panelframe/.style={draw=black!60, line width=0.25pt, rounded corners=0.8pt}}
\newcommand{\FramePanel}[1]{\draw[panelframe] (#1.north west) rectangle (#1.south east);}

% ---------- SCALE BAR ----------
\newcommand{\AddScaleBar}[3]{% #1=node, #2=length, #3=label
  \draw[line width=2pt, black]
    ($ (#1.south east) + (-1.4cm,0.28cm) $) --
    ($ (#1.south east) + (-1.4cm+#2,0.28cm) $);
  \draw[line width=1.2pt, white]
    ($ (#1.south east) + (-1.4cm,0.28cm) $) --
    ($ (#1.south east) + (-1.4cm+#2,0.28cm) $);
  \node[anchor=south, font=\scalefs, text=white, fill=black,
        inner sep=1.2pt, rounded corners=0.6pt]
    at ($ (#1.south east) + (-1.4cm+#2/2,0.62cm) $) {#3};
}

% ---------- COLORBARS (avoid overlap) ----------
\newcommand{\ampColorbar}{figures/out-dist-fig/amplitude_colorbar.png}
\newcommand{\phaseColorbar}{figures/out-dist-fig/phase_colorbar.png}
\newcommand{\IncludeCbarLeftOf}[2]{%
  \IfFileExists{#2}{%
    \node[anchor=north east] at ($(#1.north west) + (-\cbarsep,0)$)
      {\includegraphics[height=\cbarh]{#2}}; }{}%
}
\newcommand{\IncludeCbarRightOf}[2]{%
  \IfFileExists{#2}{%
    \node[anchor=north west] at ($(#1.north east) + (\cbarsep,0)$)
      {\includegraphics[height=\cbarh]{#2}}; }{}%
}

% ---------- Header & Legend styles ----------
\tikzset{
  headerbox/.style={
    draw=black!30, rounded corners=2pt, fill=white,
    align=center, inner xsep=6pt, inner ysep=4pt, font=\small\bfseries
  },
  legendbox/.style={
    draw=black!30, rounded corners=2pt, fill=white,
    inner xsep=6pt, inner ysep=6pt
  }
}

% Begin TikZ content
% Separate lifts for headers
\newlength{\hdrsepRowOne}
\setlength{\hdrsepRowOne}{4mm}   % distance above ROW 1
\newlength{\hdrsepGT}
\setlength{\hdrsepGT}{3mm}     % GT header proximity
\newlength{\gtlift}
\setlength{\gtlift}{30mm}      % raise GT pair

\begin{tikzpicture}[font=\figfs, every node/.style={inner sep=0, outer sep=0}, >=latex]

% Declare layers
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% =======================
% Top row (PINN)
% =======================
\node[anchor=north west] (p11a) at (0,0)
  {\IncludeOrPlaceholderWH{\pinnIDamp}{\subw}{\subh}};
\node[anchor=north west] (p11p) at ($(p11a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnIDphs}{\subw}{\subh}};
\node[fit=(p11a)(p11p), inner sep=0pt] (cell11) {};
% \AddScaleBar{p11a}{1cm}{X nm} \AddScaleBar{p11p}{1cm}{X nm}
\FramePanel{p11a} \FramePanel{p11p}

% Top row middle (PINN OOD)
\node[anchor=north west] (p12a) at ($(cell11.north east) + (\panelsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnOODamp}{\subw}{\subh}};
\node[anchor=north west] (p12p) at ($(p12a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnOODphs}{\subw}{\subh}};
\node[fit=(p12a)(p12p), inner sep=0pt] (cell12) {};
% \AddScaleBar{p12a}{1cm}{X nm} \AddScaleBar{p12p}{1cm}{X nm}
\FramePanel{p12a} \FramePanel{p12p}

% =======================
% Bottom row (supervised baseline)
% =======================
\node[anchor=north west] (p21a) at ($(cell11.south west) + (0,-\panelsepY)$)
  {\IncludeOrPlaceholderWH{\baseIDamp}{\subw}{\subh}};
\node[anchor=north west] (p21p) at ($(p21a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\baseIDphs}{\subw}{\subh}};
\node[fit=(p21a)(p21p), inner sep=0pt] (cell21) {};
% \AddScaleBar{p21a}{1cm}{X nm} \AddScaleBar{p21p}{1cm}{X nm}
\FramePanel{p21a} \FramePanel{p21p}

\node[anchor=north west] (p22a) at ($(p12a.south west) + (0,-\panelsepY)$)
  {\IncludeOrPlaceholderWH{\baseOODamp}{\subw}{\subh}};
\node[anchor=north west] (p22p) at ($(p22a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\baseOODphs}{\subw}{\subh}};
\node[fit=(p22a)(p22p), inner sep=0pt] (cell22) {};
% \AddScaleBar{p22a}{1cm}{X nm} \AddScaleBar{p22p}{1cm}{X nm}
\FramePanel{p22a} \FramePanel{p22p}

% =======================
% Reference column (raised)
% =======================
\node[fit=(cell11)(cell21), inner sep=0pt] (bothrows) {};
\coordinate (gt-x) at ($ (cell12.north east) + (\panelsepX,0) $);
\node[anchor=west, yshift=\gtlift] (ref-amp) at ($(bothrows.center |- gt-x)$)
  {\IncludeOrPlaceholderWH{\refAmp}{\refw}{\refh}};
\node[anchor=north west] (ref-phs) at ($(ref-amp.north east) + (\refgap,0)$)
  {\IncludeOrPlaceholderWH{\refPhs}{\refw}{\refh}};
\node[fit=(ref-amp)(ref-phs), inner sep=0pt] (refcol) {};
% Scale bars removed - pixel size TBD
% \AddScaleBar{ref-amp}{1cm}{X nm} \AddScaleBar{ref-phs}{1cm}{X nm}
\FramePanel{ref-amp} \FramePanel{ref-phs}

% Colorbars (no overlap with GT)
\IncludeCbarLeftOf{ref-amp}{\ampColorbar}
\IncludeCbarRightOf{ref-phs}{\phaseColorbar}

% =======================
% Fit nodes for rows & grid
% =======================
\node[fit=(p11a)(p11p)(p12a)(p12p), inner sep=0pt] (row1grid) {};
\node[fit=(p21a)(p21p)(p22a)(p22p), inner sep=0pt] (row2grid) {};
\node[fit=(cell11)(cell12)(cell21)(cell22), inner sep=0pt] (grid) {};

% =======================
% Gray separator line above ROW 2 (and below GT) - REMOVED
% =======================

% =======================
% Boxed headers placed correctly
% =======================
\node[headerbox, anchor=south east] (hdr1) at ($(grid.north east)+(0,\hdrsepRowOne)$) {Out-of-Distribution (Train A $\rightarrow$ Test B)};
\node[headerbox, anchor=south west] (hdr2) at ($(grid.north west)+(0,\hdrsepRowOne)$) {In-Distribution (Train B $\rightarrow$ Test B)};
\node[headerbox] (hdr3) at ($(refcol.north)+(0,\hdrsepGT)$)        {Reference (ePIE)};

% =======================
% Row labels (included in bbox)
% =======================
\node[anchor=east, font=\bfseries\figfs, text=pinncol] (rowPINN)
  at ($ (cell11.west) + (-5mm,0) $) {PtychoPINN};
\node[anchor=east, font=\bfseries\figfs, text=basecol] (rowBASE)
  at ($ (cell21.west) + (-5mm,0) $) {Supervised};

% =======================
% Top-left legend (foreground; never hidden)
% =======================
\begin{pgfonlayer}{foreground}
  % Create invisible matrix first to get positions
  \matrix (L) [matrix of nodes,
               row sep=5mm, column sep=4mm,
               ampersand replacement=\&,
               nodes={anchor=west, font=\small},
               anchor=south west] at ($ (grid.north west) + (0, \hdrsepRowOne + 12mm) $)
  {
    \node[font=\small\bfseries] {A}; \& APS-2-ID; \& \node (Aimg) {}; \\
    \node[font=\small\bfseries] {B}; \& LCLS XPP; \& \node (Bimg) {}; \\
  };
  % Legend background box fitted around the matrix & images (draw first)
  \node[legendbox, fit=(L) (Aimg) (Bimg)] (legend) {};
  % Now draw content on top
  \matrix (L) [matrix of nodes,
               row sep=5mm, column sep=4mm,
               ampersand replacement=\&,
               nodes={anchor=west, font=\small},
               anchor=south west] at ($ (grid.north west) + (0, \hdrsepRowOne + 12mm) $)
  {
    \node[font=\small\bfseries] {A}; \& APS-2-ID; \& \node (Aimg) {}; \\
    \node[font=\small\bfseries] {B}; \& LCLS XPP; \& \node (Bimg) {}; \\
  };
  % Probe thumbnails in legend
  \def\lgr{0.35cm}
  \begin{scope}
    \clip (Aimg.center) circle (\lgr);
    \node at (Aimg.center) {\IncludeOrPlaceholderWH{\probeA}{0.9cm}{0.9cm}};
  \end{scope}
  \draw[black!70, line width=0.4pt] (Aimg.center) circle (\lgr);
  \begin{scope}
    \clip (Bimg.center) circle (\lgr);
    \node at (Bimg.center) {\IncludeOrPlaceholderWH{\probeB}{0.9cm}{0.9cm}};
  \end{scope}
  \draw[black!70, line width=0.4pt] (Bimg.center) circle (\lgr);
\end{pgfonlayer}

% =======================
% Tight bounding box (include legend!)
% =======================
\node[fit=(legend)(hdr1)(hdr2)(hdr3)(rowPINN)(rowBASE)(refcol)(cell11)(cell12)(cell21)(cell22), inner sep=0pt] (ALL) {};
\begin{pgfonlayer}{background}
  \path[use as bounding box] (ALL.north west) rectangle (ALL.south east);
\end{pgfonlayer}

\end{tikzpicture}
% End TikZ content
  }% End resizebox
  \caption{Comparison of methods for an in-distribution LCLS control (train LCLS XPP, test LCLS XPP) and out-of-distribution transfer (train APS, test LCLS XPP). The reference column shows an ePIE reconstruction of the LCLS data.}
  \label{fig:fivepanel}
\end{figure}

\subsection{Computational Performance}

PtychoPINN processes approximately 6.1k diffraction patterns per second at $64\times64$ image resolution and 2.6k patterns per second at $128\times128$ on an NVIDIA RTX 3090, excluding stitching/reassembly time. The upper limit of computational performance in conventional reconstruction is SHARP, whose authors report approximately 300 frames per second per GPU at $128\times128$ ~\cite{Marchesini2016SHARP}. PtychoPINN therefore has approximately a 9-fold advantage in reconstruction throughput compared to state-of-the-art iterative solvers. (The most commonly-used conventional solvers under practical settings, such as least squares maximum likelihood (LSQ-ML) and rPIE, are notably slower than SHARP.)

\section{Discussion}

\subsection*{Physics-constrained flexibility and single-shot}
In classical ptychography, translational overlap provides real-space redundancy for phase retrieval. In our framework, the group size ($C_g$) explicitly controls overlap, so redundancy can come either from inter-measurement overlap or from structured probe phase within a single exposure. This flexibility is particularly valuable in the Fresnel (curved-probe) regime, where we observe stable single-shot reconstructions (Fig.~\ref{fig:recon_2x2}). Conventional solvers typically require overlap by construction, whereas our formulation supports reconstruction with or without overlap when the probe provides sufficient phase diversity.

\subsection*{Dose efficiency}
The Poisson NLL objective matches photon-counting statistics, providing correct likelihood scoring at low intensities where MAE is biased. Because high-$q$ diffraction signal is weak and noise-dominated, an objective that properly weights low-count pixels improves dose efficiency (Fig.~\ref{fig:dose}). This is consistent with qualitative comparisons of low-dose reconstructions (Fig.~\ref{fig:lowcounts}).

\subsection*{Generalization and data efficiency}
On the Siemens-star spatial holdout, the supervised baseline shows a clear train--test gap, while PtychoPINN maintains comparable resolution across held-out positions (Figs.~\ref{fig:smalldat}, \ref{fig:ssim}). Under a distribution shift (APS$\!\to\!$LCLS), the supervised baseline degrades markedly, whereas PtychoPINN preserves edge structure despite phase distortions (Fig.~\ref{fig:fivepanel}). Real-space supervision is susceptible to overfitting nuisance parameters---such as global phase shifts---that are intrinsic to the inverse problem, limiting transferability. The SSIM curves further indicate that PtychoPINN reaches a given fidelity with far fewer training samples (Fig.~\ref{fig:ssim}). Together, these results point to a strong inductive bias from the diffraction forward model that improves both data efficiency and transfer robustness.

\subsection*{Implications for modern light sources}
These capabilities relax overlap requirements and reduce the burden of labeled training data, enabling more flexible experimental design and faster feedback in high-rate CDI workflows. Single-shot Fresnel CDI and improved dose efficiency are particularly relevant for dynamic or radiation-sensitive samples.

\subsection*{Limitations and outlook}
Our formulation assumes a fixed, pre-estimated probe during both training and inference; joint probe retrieval within the self-supervised loop is a natural extension. Probe drift and position errors are not modeled explicitly and could be addressed by stochastic position models or differentiable refinement.

We expect optimizations such as mixed-precision execution and kernel fusion to yield a factor of $2$ in computational performance without any algorithmic / architectural adjustments.

The architecture is modular: the inverse-mapping neural network backbone, physics constraints, and loss can be modified independently. Extending the approach to Bragg CDI and reflection geometries would primarily require forward-model adaptations, with no changes to the neural network or training procedure. Of more immediate interest, we intend to replace the CNN backbone with a Fourier neural operator-inspired network to close the resolution gap between ML approaches and conventional reconstruction.

In summary, coupling a Poisson photon-counting likelihood with a differentiable forward model yields dose efficiency, robustness to sparse or irregular acquisition, and support for single-shot Fresnel CDI---unifying overlapped ptychography and single-exposure imaging within one framework and broadening the practical operating regime of coherent imaging at modern light sources.


\newpage
\section*{Appendix A: Key Configuration Parameters}
These parameters control critical aspects of the reconstruction process and should be tuned based on experimental conditions and computational constraints.
\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Model parameters, default code values, and settings used for the APS/LCLS experiments in this paper}
\label{tab:config_params}
\begin{tabular}{lcl}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Description} \\
\hline
\texttt{N} & 64 & Patch dimension (pixels) \\
\texttt{C\_g} & 1 & Patterns per group (code default: 4) \\
\texttt{K} & 7 & Nearest neighbors for grouping \\
\texttt{nsamples} & 1 & Random samplings per scan point \\
\texttt{pad\_object} & True & Enable adaptive boundary learning \\
\texttt{probe.mask} & True & Apply circular probe support \\
\texttt{gaussian\_smoothing\_sigma} & 0.0 & Probe boundary smoothing \\
\texttt{intensity\_scale.trainable} & False & Learnable intensity scaling \\
\texttt{n\_filters\_scale} & 1 & Network width multiplier \\
\texttt{amp\_activation} & sigmoid & Amplitude decoder activation \\
\texttt{offset} & 4 & Scan step size (pixels) \\
$d$ & 3-5 & Encoder depth (resolution-dependent) \\
$C$ & 132 & Total decoder channels (before split) \\
$C_{\text{latent}}$ & 128 & Latent channels at bottleneck \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Mathematical symbols and conceptual descriptions}
\label{tab:symbols}
\begin{tabular}{lcl}
\hline
\textbf{Symbol} & \textbf{Type / Structure} & \textbf{Description} \\
\hline
$x'$ & Set of $C_g$ real images & Raw diffraction patterns for one sample \\
$x$ & Set of $C_g$ real images & Normalized diffraction patterns for one sample \\
$\vec{r}_k$ & 2D Position Vector & Absolute scan position for the $k$-th image within a sample \\
$\vec{r}_{\text{global}}$ & 2D Position Vector & Centroid of a solution region (group of scans) \\
$\vec{r}^{\,\text{rel}}_k$ & 2D Offset Vector & Relative scan offset within a solution region \\
$e^{\alpha_{\log}}$ & Scalar (trainable or fixed) & Log-intensity scale parameter \\
$N_{\text{photons}}$ & Scalar & Target average total photons per diffraction pattern \\
$P(\vec{r})$ & $N \times N$ complex array & Effective probe function \\
$O_k$ & $N \times N$ complex array & $k$-th object patch decoded by the network $G$ \\
$O_{\text{region}}$ & $M \times M$ complex array & Merged object representation for a solution region \\
$O'_{\text{k}}$ & $N \times N$ complex array & Object patch extracted from $O_{\text{region}}$ for forward model \\
$\Psi_k$ & $N \times N$ complex array & Predicted complex wavefield at the detector \\
$\hat{A}_k$ & $N \times N$ real array & Predicted final diffraction amplitude for one patch \\
$\lambda_{ijk}$ & Scalar & Poisson rate parameter for a single pixel \\
\hline
\multicolumn{3}{l}{\footnotesize $N$: patch dimension, $C_g$: patches per group, $M$: merged region size}
\end{tabular}
\end{table}


\bibliographystyle{plain}
\bibliography{references}


\end{document}
