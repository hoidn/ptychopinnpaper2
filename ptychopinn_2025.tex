%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{url}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc,positioning,fit,backgrounds,matrix,arrows.meta}
\usepackage{ifthen}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{adjustbox}
\sisetup{
    round-mode = places,
    round-precision = 2,
    detect-all
}
\usepackage{xcolor}
\usepackage{multirow}
\definecolor{bestval}{RGB}{0,100,0}\usepackage{siunitx}
\sisetup{detect-all}

%% Theorem styles
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

% Color definitions for the figure
\definecolor{traincol}{RGB}{0,128,128}  % teal for TRAIN
\definecolor{testcol}{RGB}{108,0,158}   % purple for TEST
\definecolor{pinncol}{RGB}{0,95,184}    % blue for PINN row label
\definecolor{basecol}{RGB}{194,79,0}    % orange for Baseline row label
\definecolor{hdrbg}{RGB}{245,245,245}   % header ribbon bg

% Author and affiliation macros for article class
\newcommand{\fnm}[1]{#1}
\newcommand{\sur}[1]{#1}
\newcommand{\affil}[2][]{#2}

\raggedbottom

\begin{document}

% \title{An Improved Physics-Constrained Deep Neural Network For Rapid, Single-Shot Coherent Diffractive Imaging}
\title{Towards single-shot coherent imaging via overlap-free ptychography}
% or : “Towards single-shot coherent imaging via overlap-free ptychography”

% TODO choose a title
% baseline -> ptychonn / supervised ML; iterative solvers
% graphical abstract; 
% what's the broad impact?
% quantitative metrics
% donut-shaped vs gaussian probe vs disk




% Abstract:
% 1. the right physics; 2. ML; 3. probe design. 
% single-shot imaging at XFELs; better throughput at other light source
% -> high resolution operando imaging 

%Lets keep the term "Deep Neural Network".

%\author{Oliver Hoidn$^1$,  \\
%$^1$SLAC National Accelerator Laboratory, Menlo Park, California, USA \\
%\texttt{ohoidn@slac.stanford.edu}}

\date{\today}

\maketitle

\begin{abstract}
Single-shot coherent imaging is important for XFEL science because it lowers photon dose, removes the overhead of overlapping scans, and enables real-time feedback. We present a \emph{physics-constrained Deep Neural Network (DNN)} that delivers \emph{overlap-free, single-shot} reconstructions in a Fresnel (near-field) CDI geometry and also accelerates conventional multi-shot ptychography. The DNN is \emph{self-supervised}: it learns directly from raw diffraction patterns by enforcing a differentiable forward model of coherent scattering together with a Poisson photon-counting likelihood and a calibrated intensity scale. Two design choices enable robust experimental performance in both single- and multi-shot modes: (i) a \emph{dual-resolution} decoder that prevents exit-wave truncation with realistic, extended probes, and (ii) real-space constraints through coordinate-based grouping that supports arbitrary scan geometries. On APS and LCLS data, the approach reconstructs orders of magnitude faster than iterative solvers, remains accurate at low counts ($\sim\!10^4$ photons/frame) and zero overlap, achieves high fidelity with an order of magnitude less training data than supervised baselines, and generalizes across facilities without retraining. In the Fresnel regime we obtain stable \emph{single-shot} reconstructions using only the probe curvature; that is, with no beam multiplexers or modulators. This unifies single-exposure Fresnel CDI and overlapped ptychography within one computational framework, enabling \emph{high-resolution, dose-efficient operando} imaging at XFELs at substantially higher throughput than previously feasible.

\end{abstract}

% \begin{abstract}
% Ptychographic Coherent Diffractive Imaging (CDI) underpins nanoscale imaging at synchrotron and X-ray free-electron laser facilities, but computational bottlenecks prevent real-time reconstruction. We present a physics-informed neural network (PINN) that reconstructs experimental diffraction data from the Linac Coherent Light Source (LCLS) and Advanced Photon Source (APS) 100-1000$\times$ faster than conventional iterative algorithms. The method handles challenging experimental realities---stage jitter, low photon flux ($10^4$ photons/frame), and sparse scanning (sub-$10\%$ overlap)---where conventional algorithms fail to converge or require careful position correction. Our self-supervised framework trains directly on experimental diffraction patterns without requiring ground-truth images and can generalize from as few as 250 training samples compared to several thousand for comparable quality with supervised approaches.  We also find that the framework produces capable single-shot, overlap-free reconstructions in Fresnel CDI configurations--a surprising capability that is absent in conventional ptychography solvers. These advances will make real-time coherent imaging feedback feasible at high-repetition-rate light sources such as the LCLS-II-HE.
% \end{abstract}
% TODO deleted from abstract: We demonstrate cross-facility generalization, reconstructing LCLS samples using models trained solely on APS data.

% \section{Introduction}\label{sec1}
% %Central theme: There is a widening gap between the rate of data genenration, and that of scientific analyses. This gap needs to be addressed. 
 
% Modern light sources, such as fourth-generation synchrotrons and X-ray Free-Electron Lasers (XFELs), now generate coherent diffraction data at unprecedented rates, often reaching terabytes per hour (Figure \ref{fig:Figure1}). This data deluge poses a significant challenge for Coherent Diffraction Imaging (CDI) techniques like ptychography, as traditional image reconstruction relies on computationally intensive iterative phase retrieval algorithms. An analytical bottleneck has emerged where offline processing time vastly exceeds data acquisition time. This disparity not only delays scientific insights but, more critically, precludes real-time feedback and on-the-fly experimental steering—capabilities essential for maximizing the efficiency and discovery potential of these facilities. Consequently, there is a pressing need for new reconstruction paradigms that deliver high-fidelity results at accelerated rates without sacrificing reliability or robustness.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/Figure1.pdf}
%     \caption{Estimates of raw data generation rate for light sources over time \emph{maybe move this to SI section}}
%     \label{fig:Figure1}
% \end{figure}

 
% Ptychographic coherent diffractive imaging is a cornerstone technique for nanoscale imaging at synchrotron and XFEL facilities, achieving sub-10 nm resolution beyond the limitations of X-ray optics~\cite{GuizarSicairos2021PhysicsToday}. However, classical iterative algorithms like the Ptychographic Iterative Engine (PIE) require $\sim 60-70\%$ scan overlap for robust convergence and can only process $\sim 0.1-1$ diffraction patterns per second on standard hardware~\cite{Bunk2008Overlap,Maiden2009UltramicroscopyPIE}. At high-repetition-rate sources (e.g., LCLS-II, MHz-class), data acquisition outpaces reconstruction by orders of magnitude~\cite{LCLSIIHE_DesignPerf}. Consequently, even highly-optimized GPU/HPC solvers require substantial infrastructure and struggle to provide the low-latency feedback needed for interactive experiments~\cite{Marchesini2016SHARP,Babu2023EdgePtycho}.

% To address this challenge, machine learning (ML) has been explored to accelerate reconstruction. Supervised ML methods, for instance, can achieve significant speedups\cite{Cherukara2020PtychoNN}. However, their practical application is limited. Fundamentally, they treat reconstruction as a black-box mapping from diffraction data to image space, often failing to generalize because they do not explicitly enforce the physical constraints of diffraction. This leads to poor performance when transferring between different samples or experimental conditions, and a high sensitivity to unmodeled artifacts like probe instability or position errors\cite{Maiden2012Annealing,Zhang2013Position,Seifert2023PoissonGaussian, [ptychonn nat com 'machine learning at the edge' natcom paper as example of the impracticality of these approaches]}. Moreover, these methods impose the onerous burden of generating large, curated datasets of ground-truth reconstructions for training. Our previous work, PtychoPINN~\cite{Hoidn2023PtychoPINN}, showed that a Physics-Informed Neural Network (PINN) could achieve rapid, self-supervised reconstruction. However, its effectiveness was demonstrated only on synthetic data with idealized probes and regular scan patterns. 
% % In parallel, maximum-likelihood formulations grounded in photon counting provide a principled objective for low-dose data~\cite{Thibault2012NJPML}. 

% Here, we overcome the limitations of the original PtychoPINN by introducing several key advances that enable robust reconstruction of experimental data. Specifically, we extend the framework by: 
% \begin{enumerate}
%     \item  modeling realistic, extended probe functions; 
%     \item handling arbitrary and irregular scan patterns through nearest-neighbor clustering; 
%     \item  incorporating a Poisson likelihood to accurately model photon-counting statistics~\cite{Thibault2012NJPML,Seifert2023PoissonGaussian};   \item  demonstrating cross-facility generalization, where a model trained on data from one beamline can reconstruct data from another. 
% \end{enumerate}

% Collectively, these advances transition PtychoPINN from a proof-of-concept into a practical tool, establishing a viable path toward low-latency feedback and analysis at high-repetition-rate light sources.
% % TODO probably remove point iv bc it overstates things and is also not really part of this paper's focus

% Beyond acceleration, our physics-informed approach enables imaging modalities previously inaccessible to iterative solvers. We demonstrate this with single-shot, overlap-free reconstruction in a Fresnel CDI configuration\cite{Williams2006FresnelCDI,Zhang2016CMI}, a method that maximizes throughput by eliminating the scanning overlap required by conventional ptychography\cite{Bunk2008Overlap}. Such a capability is critical for studying fast dynamics and minimizing radiation dose on sensitive biological specimens, addressing two major challenges in modern X-ray imaging.

\section{Introduction}\label{sec1}
% Coherent diffractive imaging (CDI) at modern synchrotrons and X‑ray free‑electron lasers (XFELs) enables nanoscale measurements but remains constrained by computation: data are generated far faster than images can be reconstructed, limiting real‑time feedback ~\cite{GuizarSicairos2021PhysicsToday,LCLSIIHE_DesignPerf}. Classical solvers such as ePIE and iterative least-squares maximum likelihood are typically too slow to keep pace with high‑repetition‑rate sources even with GPU/HPC acceleration~\cite{Marchesini2016SHARP,Babu2023EdgePtycho}. They also depend on substantial translational overlap ($\sim$60--70\%) for robust convergence~\cite{Bunk2008Overlap,Maiden2009UltramicroscopyPIE}, which inflates the time and computational resources required to image a given 

% Another dimension of the problem is Single‑exposure imaging would remove scanning overhead, reduce dose, and make each XFEL pulse an analyzable event. Here we use ``single‑shot'' in the \emph{Fresnel} (near‑field) sense: a curved/defocused probe provides phase diversity sufficient, in principle, to reconstruct a complex image from a single diffraction measurement—without lateral scanning, beam multiplexing, or a downstream modulator~\cite{Williams2006FresnelCDI}. This distinguishes our goal from \emph{multiplexed single‑shot ptychography}, which acquires many overlapping positions in parallel using beam‑splitting gratings or arrays~\cite{Sidorenko2015Optica,Kharitonov2022SciRep}, and from \emph{coherent modulation imaging} (CMI), which inserts a modulator to encode wavefront diversity for single‑exposure phase retrieval~\cite{Zhang2016CMI,Dong2018CMI}. Related near‑field/ptychographic formulations leverage probe curvature but still scan multiple positions~\cite{Stockmar2013Nearfield,Vine2009PFCDI}.

% We introduce a physics‑constrained, self‑supervised \emph{Deep Neural Network} (DNN) that learns directly from raw diffraction by enforcing a differentiable forward model of coherent scattering together with a Poisson photon‑counting likelihood~\cite{Thibault2012NJPML,Seifert2023PoissonGaussian}. Two design choices make the method robust under experimental conditions: a dual‑resolution decoder that represents the object sharply in the central, well‑posed region while extending into the periphery to avoid exit‑wave truncation with realistic, extended probes (consistent with oversampling constraints~\cite{miao1999extending}); and spatial sample-grouping that accommodates irregular, sparse scan geometries without assuming a Cartesian raster. Notably, Fresnel geometry alters the conditioning of the inverse problem: the reciprocal‑space likelihood becomes the primary anchor, and real‑space overlap becomes optional rather than mandatory. Under this approach, single-shot imaging simply corresponds to removal of the optional overlap constraints. 

% Compared to supervised inference (e.g., PtychoNN)~\cite{Cherukara2020PtychoNN,Babu2023EdgePtycho}, this self‑supervised, physics‑guided framework avoids label generation, improves data/dose efficiency, and exhibits superior in- and out-of-sample generalization. It builds on a previous prototype~\cite{Hoidn2023PtychoPINN} by extending it to realistic probes and arbitrary scan geometries.

% On experimental datasets from the Advanced Photon Source (APS) and the Linac Coherent Light Source (LCLS), the method yields accurate reconstructions at low photon counts and zero overlap while delivering large speedups over iterative solvers~\cite{Marchesini2016SHARP}. Most notably, in Fresnel configurations we obtain stable \emph{single‑shot} reconstructions—using only probe curvature—without beam multiplexers~\cite{Sidorenko2015Optica,Kharitonov2022SciRep} or downstream modulators~\cite{Zhang2016CMI,Dong2018CMI}. Unifying single‑exposure Fresnel CDI and overlapped ptychography within one computational framework enables dose‑efficient, high‑throughput imaging and moves real‑time feedback within reach at high‑rate XFELs~\cite{LCLSIIHE_DesignPerf}. Detailed evaluations, ablations, and comparisons are provided in the Results and Methods.

%Central theme: There is a widening gap between the rate of data genenration, and that of scientific analyses. This gap needs to be addressed. 

%----------------------------
%-----TODOs--------------
%STAGE 1: 29th December
%[OH]Figure 2: Add OOD panel comparing in dist accu to ood accu for both models [Jan 7th]
%[OH]Figure 3: Replace baseline with MLE.
%[OH]Figure 4: Explicate labels for baseline.
%[OH]Figure 5: Tweak: labels on colorbar, labeling.
%[AAM-DONE]Change placements of Figures 6 and 7 [make SM].
%[AAM-DONE]Figure 7: Add beaucoup de references.
%[AAM and OH]: Work on the text.

%Stage 2:
%Re-work the text.
%----------------------------
 
Modern light sources, such as fourth-generation synchrotrons and X-ray Free-Electron Lasers (XFELs), now generate coherent diffraction data at unprecedented rates, often reaching terabytes per second. This data deluge poses a significant challenge for Coherent Diffraction Imaging (CDI) techniques like ptychography, as traditional image reconstruction relies on computationally intensive iterative phase retrieval algorithms. An analytical bottleneck has emerged where offline processing time vastly exceeds data acquisition time. This disparity not only delays scientific insights but, more critically, precludes real-time feedback and on-the-fly experimental steering—capabilities essential for maximizing the efficiency and discovery potential of these facilities. Consequently, there is a pressing need for new reconstruction paradigms that deliver high-fidelity results at accelerated rates without sacrificing reliability or robustness.

Ptychographic coherent diffractive imaging is a cornerstone technique for x-ray nanoscale imaging at synchrotron and XFEL facilities ~\cite{GuizarSicairos2021PhysicsToday}. However, classical iterative algorithms like the Ptychographic Iterative Engine (PIE) require $\sim 60-70\%$ scan overlap for robust convergence and can only process $\sim 0.1-1$ diffraction patterns per second on standard hardware~\cite{Bunk2008Overlap,Maiden2009UltramicroscopyPIE}. At high-repetition-rate sources (e.g., LCLS-II, MHz-class), data acquisition outpaces reconstruction by orders of magnitude~\cite{LCLSIIHE_DesignPerf}. Consequently, even highly-optimized GPU/HPC solvers require substantial infrastructure and struggle to provide the low-latency feedback needed for interactive experiments~\cite{Marchesini2016SHARP,Babu2023EdgePtycho}.

To address this challenge, machine learning (ML) has been explored to accelerate reconstruction. Supervised convolutional networks, for instance, can achieve significant speedups\cite{Cherukara2020PtychoNN}. However, their practical application is limited by poor generalization and resolution~\cite{Maiden2012Annealing,Zhang2013Position,Seifert2023PoissonGaussian}. Moreover, these methods impose the onerous burden of generating large, curated datasets of ground-truth reconstructions for training. Our previous work showed that a Physics-Informed Neural Network (PINN) can achieve rapid, self-supervised reconstruction. However, this was demonstrated only on synthetic data with idealized probes and regular scan patterns. 
% In parallel, maximum-likelihood formulations grounded in photon counting provide a principled objective for low-dose data~\cite{Thibault2012NJPML}. 

Here, we extend the original framework with several improvements that enable robust reconstruction of actual experimental data. Specifically, we add support for long-tailed probe functions and arbitrary scan patterns.

We additionally explore this framework's capability for \emph{single‑shot} reconstructions—using only probe curvature—without beam multiplexers~\cite{Sidorenko2015Optica,Kharitonov2022SciRep} or downstream modulators~\cite{Zhang2016CMI,Dong2018CMI}. Unifying single‑exposure Fresnel CDI and overlapped ptychography within one computational framework enables dose‑efficient, high‑throughput imaging and brings real‑time feedback within reach at high‑rate XFELs~\cite{LCLSIIHE_DesignPerf}. 

% Beyond acceleration, our physics-informed approach enables imaging modalities previously inaccessible to iterative solvers. We demonstrate this with single-shot, overlap-free reconstruction in a Fresnel CDI configuration\cite{Williams2006FresnelCDI,Zhang2016CMI}, a method that maximizes throughput by eliminating the scanning overlap required by conventional ptychography\cite{Bunk2008Overlap}. Such a capability is critical for studying fast dynamics and minimizing radiation dose on sensitive biological specimens, addressing two major challenges in modern X-ray imaging.

%Single‑exposure CDI under coded illumination has two main realizations. In Fresnel CDI, a curved probe supplies phase diversity, enabling single‑exposure reconstructions without lateral scanning; this was demonstrated early in the X‑ray regime and extended to tomography. 
 % In coherent modulation imaging (CMI), a known modulator near the sample encodes the wavefront; CMI and its broadband variants have emphasized single‑shot capability at pulsed sources, albeit with additional optics and calibration demands. 
 % By contrast, single‑shot ptychography often refers to hardware multiplexing—a grating or beam array creates many overlapping probes recorded in one exposure—which has been demonstrated at optical wavelengths and, more recently, at soft‑X‑ray FELs. 
 % Our work targets the strict one‑frame limit with no overlap and no modulator in Fresnel geometry, and achieves stable reconstructions via a self‑supervised, physics‑constrained learner that optimizes a Poisson photon‑counting likelihood, building on ML formulations of maximum‑likelihood CDI/ptychography.


\begin{figure}[t]
  \centering
  % Row 1: Idealized
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/idealized_cdi_scaled_v5_small.png}
    \caption{Idealized — CDI}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/idealized_ptycho_scaled_v5_small.png}
    \caption{Idealized — Ptycho}
  \end{subfigure}

  \vspace{0.6em}

  % Row 2: Hybrid
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hybrid_cdi_scaled_v5_small.png}
    \caption{Hybrid — CDI}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hybrid_ptycho_scaled_v5_small.png}
    \caption{Hybrid — Ptycho}
  \end{subfigure}
% TODO: add small images of the probe phases as insets
  \caption{Reconstruction comparison. Rows: Idealized vs Hybrid probe. Columns: CDI vs Ptycho.}
  \label{fig:recon_2x2}
\end{figure}

\paragraph{Inverse problem and constraints.}
Coherent diffractive imaging requires the recovery of a complex object from intensity-only diffraction measurements. Reconstruction methods enforce two complementary constraints: a reciprocal-space constraint requiring predicted intensities to match data (via a physics-based forward model), and a real-space constraint enforcing consistency between overlapping views. In our framework, the reciprocal-space constraint is enforced directly via a differentiable forward model and a Poisson likelihood. Real-space overlap is handled via a translation-aware merging operator. Crucially, this allows overlap to be treated as a flexible experimental parameter rather than a hard requirement; setting the group size to a single frame ($C_g = 1$) removes overlap constraints entirely, enabling ``single-shot'' reconstruction when the probe provides sufficient phase diversity. 

\section{Methods and Architecture}
\label{sec:methods}

We train a physics-informed neural network (PINN) to perform self-supervised ptychographic reconstruction by composing a learned inverse map with a differentiable forward model of coherent scattering. This section consolidates the formulation, operators, normalization, network design (including extended-probe handling), and training objective.

\subsection{Formulation and Forward Model}
\label{sec:formulation}

We learn an inverse map $G: X \!\to\! Y$ from diffraction space to real space and optimize it by composing with a differentiable forward model $F: Y \!\to\! X$. The overall autoencoder is $F \circ G$, trained to match measured diffraction statistics without ground-truth images.

\paragraph{Data model and notation.}
Each training sample comprises $C_g$ diffraction amplitude images $\{x_k\}_{k=1}^{C_g}$ acquired at probe coordinates $\{\vec{r}_k\}_{k=1}^{C_g}$. The network $G(x,r)$ outputs $C_g$ complex object patches $\{O_k\}_{k=1}^{C_g}$ on an $N\times N$ grid.  We use:
\begin{itemize}\itemsep3pt
  \item $\mathcal{T}_{\Delta \vec{r}}[\cdot]$: real-space translation by $\Delta \vec{r}$,
  \item $\mathrm{Pad}[\cdot]$: zero-padding to a canvas large enough to contain all translated patches,
  \item $\mathrm{Pad}_{N/4}[\cdot]$: zero-padding that embeds a central $N/2\times N/2$ tile into an $N\times N$ grid,
  \item $\mathrm{Crop}_N[\cdot]$: center-cropping to $N\times N$,
  \item $\mathbf{1}$: an all-ones array of appropriate size,
  \item $\odot$: elementwise (Hadamard) product.
\end{itemize}

\paragraph{Constraint map ($F_c$): translation-aware merging.}
To enforce overlap consistency, per-patch reconstructions are merged in a translation-aligned frame:
\begin{align}
  O_{\text{region}}(\vec{r})
  \;=\;
  \frac{
    \sum_{k=1}^{C_g}\; \mathcal{T}_{-\vec{r}_k}\!\left[\mathrm{Pad}\!\left(O_k\right)\right]
  }{
    \sum_{k=1}^{C_g}\; \mathcal{T}_{-\vec{r}_k}\!\left[\mathrm{Pad}\!\left(\mathbf{1}\right)\right] + \epsilon
  },
  \qquad \epsilon=10^{-3}.
  \label{eq:constraintmap}
\end{align}
This "translational pooling'' applies to arbitrary scan geometries.

\paragraph{Coordinate-aware grouping.}
Training groups are formed locally by nearest-neighbor sampling. For each anchor $\vec{r}_i$, let $\mathcal{N}_K(\vec{r}_i)$ be its $K$ nearest distinct neighbors. A group $\mathcal{G}_{i,j}$ draws $C_g-1$ neighbors uniformly without replacement:
\[
\mathcal{G}_{i,j}=\{\vec{r}_i\}\cup S_{i,j},\quad S_{i,j}\subset \mathcal{N}_K(\vec{r}_i),\; |S_{i,j}|=C_g-1,
\]
repeated $n_{\text{samples}}$ times per anchor. If duplicate neighbor sets are disallowed, the effective number of distinct groups per anchor is
\[
n_{\text{eff}} \;=\; \min\!\left(n_{\text{samples}},\, \binom{K}{C_g - 1}\right),
\]
so the total number of training examples is $N_{\text{scan}} \times n_{\text{eff}}$, with the combinatorial upper bound $N_{\text{scan}} \binom{K}{C_g - 1}$. Choosing $n_{\text{samples}} > 1$ augments the dataset through combinatorial re-grouping while preserving local spatial consistency.

Coordinates within each group are expressed in a stable local frame by re-centering to the group centroid
\[
\vec{r}_{\text{global}}=\frac{1}{C_g}\sum_{k=1}^{C_g}\vec{r}_k,\qquad
\vec{r}^{\,\text{rel}}_k=\vec{r}_k-\vec{r}_{\text{global}}.
\]

\paragraph{Diffraction map ($F_d$): coherent scattering.}
Given $O_{\text{region}}$, the $k$th translated object patch and exit wave are
\begin{align}
  O'_k(\vec{r}) &= \mathrm{Crop}_N\!\left[\mathcal{T}_{\vec{r}^{\,\text{rel}}_k}\!\left(O_{\text{region}}\right)\right], \\
  \Psi_k &= \mathcal{F}\!\left\{ O'_k(\vec{r}) \cdot P(\vec{r}) \right\},
\end{align}
where $P(\vec{r})$ is the (estimated) probe and $\mathcal{F}$ is the 2D Fourier transform. Predicted detector-plane amplitudes include a global intensity scale $e^{\alpha_{\log}}$ that links normalized network outputs to physical photon counts:
\begin{align}
  \hat{A}_k \;=\; |\Psi_k|\; e^{\alpha_{\log}}.
\end{align}

\subsection{Data Preprocessing}
\label{sec:preprocess}

A dataset consists of diffraction images from one or more objects measured with a fixed probe illumination $P$. After grouping images into overlapping samples (Section~\ref{sec:formulation}), we normalize the raw diffraction amplitudes to ensure numerical stability during training:
\begin{align}
  x_k \;=\; x'_k \cdot \sqrt{\frac{(N/2)^2}{\big\langle\sum_{i,j} |x'_{ij}|^2\big\rangle}},
  \label{eq:norm}
\end{align}
where $x'$ denotes raw measurements and the average is over all images in the dataset. This choice ensures order-unity activations in the neural network: by Parseval's theorem, unit-amplitude real-space objects produce diffraction power of approximately $N^2/4$, so this normalization maps experimental amplitude images to internal activations suited to gradient-based optimization.
%\footnote{The factor of 4 arises from the oversampling requirement that the reconstructed object occupies the central $N/2 \times N/2$ region of an $N \times N$ grid.}

Additionally, we introduce a trainable scalar $\alpha_{\log}$ that converts between the dimensionless internal model activations and per-pixel integrated amplitudes. As discussed in Section~\ref{sec:loss}, the role of $\alpha_{\log}$ is to convert the output \emph{intensity} into physical units of photons per pixel. The final, scaled, network input is $x_{\text{in}} = x \cdot e^{-\alpha_{\log}}$.

\subsection{Neural Network Architecture}
\label{sec:nn}

The inverse map $G$ follows an encoder–decoder design (as in \cite{Hoidn2023PtychoPINN}), conditioned on $\{x_k\}_{k=1}^{C_g}$ and $\{\vec{r}^{\,\text{rel}}_k\}_{k=1}^{C_g}$, and outputs complex patches $\{O_k\}_{k=1}^{C_g}$. To respect oversampling while avoiding truncation artifacts from realistic probes with extended tails, the decoder allocates most capacity to the central, well-posed region and a lightweight continuation to the periphery.

\paragraph{Extended probe illumination (dual-resolution decoding).}
CNN architectures are limited to modest dimensions ($N \leq 128$) and we must furthermore restrict high-resolution reconstruction to the central $N/2 \times N/2$ region to satisfy oversampling conditions \cite{miao1999extending}. Probes with extended tails force inefficient use of this limited number of pixels because the real-space area brightly illuminated by the probe is small compared to the total probe area that must be represented to avoid truncation artifacts from non-zero amplitude at the edge of the real-space grid.

Consequently, given the modest magnitude of $N$, fully inscribing the probe—tails included—within the central $N/2 \times N/2$ pixels may require too much binning. This causes a dilemma: one must choose between truncation artifacts (and possible lack of convergence due to the associated physical inconsistency) and violation of the diffraction-space oversampling condition for coherent imaging.

We resolve this by reconstructing the object in high resolution in the central $N/2 \times N/2$ region of the real-space grid and low resolution in the periphery. Presuming the absence of high spatial frequency components in the probe tail, extending the probe times object reconstruction into the periphery does not compromise well-posedness of the inverse problem.

Concretely, we use most channels ($C-4$) of the penultimate decoder layer for the central region and the remaining 4 channels to coarsely reconstruct the periphery:
\begin{align}
  O_{\mathrm{amp}} &= \mathrm{Pad}_{N/4}\!\big(\sigma_A(\mathrm{Conv}(H^{\text{central}}_A))\big)\;+\;
  \sigma_A(\mathrm{ConvUp}(H^{\text{border}}_A))\odot M_{\text{border}},\\
  O_{\mathrm{phase}} &= \mathrm{Pad}_{N/4}\!\big(\pi \tanh(\mathrm{Conv}(H^{\text{central}}_\phi))\big)\;+\;
  \pi \tanh(\mathrm{ConvUp}(H^{\text{border}}_\phi))\odot M_{\text{border}},\\
  O_k &= O_{\mathrm{amp}}\cdot \exp\!\big(i\,O_{\mathrm{phase}}\big),
\end{align}
where $H^{\text{central}}_{\{\cdot\}}$ (the first $C-4$ channels) targets the central region, $H^{\text{border}}_{\{\cdot\}}$ (the last 4 channels) produces a low-resolution continuation, and $M_{\text{border}}$ is a binary mask that isolates the boundary contributions to the outer region. This modification avoids artifacts from truncation of the exit wave and enables stable reconstruction with experimentally realistic probes.

\subsection{Training Objective and Optimization}
\label{sec:loss}

\paragraph{Poisson negative log-likelihood.}
The training procedure optimizes the inverse map $G$ using a negative log-likelihood loss under Poisson statistics:
\begin{align}
  \mathcal{L}_{\text{Poiss}}
  \;=\;
  -\sum_{k,i,j} \log f_{\text{Poiss}}(N_{kij}; \lambda_{kij})
  \;=\;
  \sum_{k,i,j}
  \left(
    \lambda_{kij} - N_{kij}\,\log\lambda_{kij}
  \right),
\end{align}
where $N_{kij} = |x'_{kij}|^2$ is the measured photon count and $\lambda_{kij}=|\hat{A}_{kij}|^2$ is the predicted count.

Since the network operates on normalized inputs (Eq.~\ref{eq:norm}) for numerical stability, a scale parameter $e^{\alpha_{\log}}$ bridges normalized and physical units. When the mean photon flux $N_{\text{photons}}$ is known, we initialize:
\begin{align}
  e^{\alpha_{\log}} \;\leftarrow\; \frac{2\sqrt{N_{\text{photons}}}}{N}.
  \label{eq:alphaloginit}
\end{align}
This ensures predicted intensities match measurement statistics. The parameter $e^{\alpha_{\log}}$ may be fixed or learned (see Table~\ref{tab:config_params}); learning it can absorb modest calibration errors.

\paragraph{Amplitude loss for unknown counts.}
For datasets lacking absolute photon counts, we resort to mean absolute error on normalized amplitudes:
\[
\mathcal{L}_{\text{MAE}}=\sum_{k,i,j} \big|x_{kij} - \hat{A}_{kij}e^{-\alpha_{\log}}\big|.
\]

In the results reported here we do not use any real-space loss; training is driven solely by the diffraction-domain losses (Poisson NLL or MAE, depending on count calibration).

\paragraph{Implementation notes.}
All operators in $F_c$ and $F_d$ are differentiable and implemented with padding-aware translations and FFT-based diffraction. Batching is performed over groups $\mathcal{G}_{i,j}$; nearest-neighbor sampling with $n_{\text{samples}}>1$ provides dataset augmentation while preserving local spatial consistency. Default architectural and training hyperparameters are summarized in Table~\ref{tab:config_params}.

\subsection{Datasets and Evaluation Protocol}
We evaluate on APS Velociprobe Siemens-star data, LCLS-XPP Run1084, and the fly001-synthetic dataset simulated from APS Siemens-star reconstructions (ground truth for Table~\ref{tab:results_16384}). For the Siemens-star experiments, we use a spatial holdout: the top half of the scan is used for training and the bottom half for testing. For out-of-distribution transfer, models trained on APS data are evaluated on LCLS data without retraining, with beamline-specific forward parameters (probe/geometry) substituted at inference.


\section{Results}

We report results on APS Velociprobe Siemens-star data, LCLS-XPP Run1084, and the fly001-synthetic dataset; see Methods for dataset definitions and evaluation protocol.

\subsection{Reconstruction Quality}

Figure~\ref{fig:smalldat} compares reconstructions on APS Siemens-star data across two sampling budgets (512 and 8192 diffraction patterns). The physics-informed model preserves clear edge structure and contrast in both regimes, while the supervised baseline (PtychoNN) degrades visibly at lower data availability. On the fly001-synthetic dataset (simulated from APS Siemens-star reconstructions), the physics-informed model also attains higher phase fidelity than the supervised baseline; see Table~\ref{tab:results_16384}.


\begin{table}[htbp]
\centering
\caption{Reconstruction quality metrics at maximum training set size (16,384 images). Values shown are mean $\pm$ standard deviation across 5 trials. Best values per dataset are highlighted in \textcolor{bestval}{green}.}
\label{tab:results_16384}
\begin{tabular}{@{}llcccc@{}}
\toprule
& & \multicolumn{2}{c}{PSNR (dB)} & \multicolumn{2}{c}{MS-SSIM} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
Dataset & Method & Amplitude & Phase & Amplitude & Phase \\
\midrule
\multirow{2}{*}{fly001-synthetic} 
& PtychoNN (supervised) & 
  $84.83 \pm 0.23$ & 
  $68.62 \pm 0.02$ & 
  $0.930 \pm 0.002$ & 
  $0.912 \pm 0.003$ \\
& PINN (self-supervised) & 
  \textcolor{bestval}{$\mathbf{85.53 \pm 0.02}$} & 
  \textcolor{bestval}{$\mathbf{70.54 \pm 0.06}$} & 
  \textcolor{bestval}{$\mathbf{0.955 \pm 0.001}$} & 
  \textcolor{bestval}{$\mathbf{0.962 \pm 0.001}$} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/lowcounts.png}
        \caption{512 diffraction patterns of the Siemens star test pattern.}
        \label{fig:lowcounts}
    \end{subfigure}
    
    \vspace{0.5em}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/8192.png}
        \caption{8192 diffraction patterns of the Siemens star test pattern.}
    \end{subfigure}
    \caption{Comparison of reconstruction quality with different numbers of diffraction patterns.}
    \label{fig:smalldat}
\end{figure}

% % Compact version for two-column format
% \begin{table}[htbp]
% \centering
% \caption{Reconstruction metrics at 16,384 training images (mean $\pm$ std, n=5)}
% \label{tab:results_compact}
% \begin{tabular}{@{}llcccc@{}}
% \toprule
% Dataset & Method & \multicolumn{2}{c}{PSNR (dB)} & \multicolumn{2}{c}{MS-SSIM} \\
% & & Amp. & Phase & Amp. & Phase \\
% \midrule
% \multirow{2}{*}{fly001-syn.} 
% & Base. (sup.) & 84.83(23) & 68.62(2) & 0.930(2) & 0.912(3) \\
% & PINN (self.) & \textbf{85.53(2)} & \textbf{70.54(6)} & \textbf{0.955(1)} & \textbf{0.962(1)} \\
% \bottomrule
% \end{tabular}
% \end{table}


\subsection{Overlap-Free Reconstruction}

In overlap-free operation, we set the group size to a single diffraction frame ($C_g = 1$), removing overlap-based real-space consistency. Reconstruction then relies entirely on the diffraction likelihood and the known probe structure (defocused probe/Fresnel geometry). Figure~\ref{fig:recon_2x2} illustrates this single-frame mode compared with multi-position ptychography. Quantitative comparisons across overlap and probe-structuring variants on a synthetic line-pattern dataset are summarized in Table~\ref{tab:sim_lines_metrics} (overlap-free $C_g=1$ vs overlap $C_g=2$).
\begin{table}[htbp]
\centering
\caption{Synthetic line-pattern reconstruction metrics on the test split. Ground truth uses the simulated object. Phase FRC50 is undefined because the ground-truth phase is constant zero.}
\label{tab:sim_lines_metrics}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Case & MAE (Amp) & MAE (Phase) & PSNR (dB) (Amp) & PSNR (dB) (Phase) & SSIM (Amp) & SSIM (Phase) & MS-SSIM (Amp) & MS-SSIM (Phase) \\
\midrule
overlap-free ($C_g=1$) + idealized probe & 0.185359 & 0.295667 & 60.672510 & 55.247873 & 0.620232 & 0.410153 & 0.852410 & 0.256867 \\
overlap-free ($C_g=1$) + custom probe & 0.072003 & 0.077751 & 68.886477 & 62.996489 & 0.904422 & 0.885615 & 0.980933 & 0.576148 \\
overlap ($C_g=2$) + idealized probe & 0.051782 & 0.007872 & 71.341465 & 86.530175 & 0.951729 & 0.996137 & 0.992623 & 0.327948 \\
overlap ($C_g=2$) + custom probe & 0.040176 & 0.045158 & 73.027381 & 64.539845 & 0.967679 & 0.965757 & 0.993833 & 0.748855 \\

\bottomrule
\end{tabular}
\end{table}

\subsection{Photon-Limited Performance}

Figure~\ref{fig:dose} summarizes resolution (FRC50) as a function of photon dose for Poisson NLL versus MAE. At $\sim10^4$ photons/frame, the Poisson loss preserves high-$q$ detail relative to MAE, consistent with the qualitative comparisons in Fig.~\ref{fig:lowcounts}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/poisson.png}
    \caption{Resolution (FRC50) as a function of on-sample photon dose for two variants of the PtychoPINN framework trained with mean absolute error (MAE) and Poisson negative log likelihood (NLL) reconstruction penalties in the self-supervised loss function}
    \label{fig:dose}
\end{figure}

\subsection{Data Efficiency}

Figure~\ref{fig:ssim} illustrates the reconstruction quality (phase SSIM) as a function of dataset size. The physics-informed framework maintains high fidelity (SSIM $> 0.95$) even when trained on as few as 512 diffraction patterns. In contrast, the supervised baseline degrades rapidly below 1024 samples. The horizontal shift between the curves indicates that PtychoPINN achieves comparable quality using roughly an order of magnitude less training data than the supervised approach. This confirms that enforcing the diffraction forward model acts as a powerful regularizer, reducing the number of samples required to constrain the solution. 

% [CUT per revision plan A1: Sparse Sampling Robustness subsection removed - content folded into Data Efficiency discussion]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ssim.png}
    \caption{Structural similarity of PtychoPINN, conventional reconstruction (rPIE in Tike), and the PtychoNN baseline as a function of training set size.}
    \label{fig:ssim}
\end{figure}

\subsection{Out-of-distribution Generalization}

To evaluate out-of-distribution generalization, we train on APS Velociprobe data and reconstruct LCLS-XPP Run1084 without retraining, using beamline-specific forward parameters (probe/geometry). Despite substantial differences in illumination profiles, the physics-informed model retains recognizable LCLS object features (Fig.~\ref{fig:fivepanel}, PINN row), while the supervised baseline degrades markedly. The distribution shift induces phase distortion and reduced quantitative fidelity, but edge structure remains visible in the PINN reconstructions. The reference column in Fig.~\ref{fig:fivepanel} is an ePIE reconstruction of the LCLS data.

\begin{figure}[t]
  \centering
  \resizebox{0.85\textwidth}{!}{% Scale to 85% of text width
    %\input{figures/outdist_figure.tex}
    % TikZ figure for inclusion in main document
% Required packages should already be loaded by main document

% ---------- GRAPHIC PATH ----------
\graphicspath{{figures/out-dist-fig/}{figures/}{./}}

% ---------- COLORS ----------
\definecolor{traincol}{RGB}{0,128,128}
\definecolor{testcol}{RGB}{108,0,158}
\definecolor{pinncol}{RGB}{0,95,184}
\definecolor{basecol}{RGB}{194,79,0}

% ---------- FILE MACROS ----------
\newcommand{\pinnIDamp}{figures/out-dist-fig/pinn_run1084_amplitude.png}
\newcommand{\pinnIDphs}{figures/out-dist-fig/pinn_run1084_phase.png}
\newcommand{\pinnOODamp}{figures/out-dist-fig/pinn_fly64trained_run1084test_amplitude.png}
\newcommand{\pinnOODphs}{figures/out-dist-fig/pinn_fly64trained_run1084test_phase.png}
\newcommand{\baseIDamp}{figures/out-dist-fig/baseline_run1084_amplitude.png}
\newcommand{\baseIDphs}{figures/out-dist-fig/baseline_run1084_phase.png}
\newcommand{\baseOODamp}{figures/out-dist-fig/baseline_fly64trained_run1084test_amplitude.png}
\newcommand{\baseOODphs}{figures/out-dist-fig/baseline_fly64trained_run1084test_phase.png}
\newcommand{\refAmp}{figures/out-dist-fig/ground_truth_run1084_amplitude.png}
\newcommand{\refPhs}{figures/out-dist-fig/ground_truth_run1084_phase.png}
\newcommand{\probeA}{figures/out-dist-fig/fly64_probe_amplitude.png}   % A: APS-2-ID
\newcommand{\probeB}{figures/out-dist-fig/run1084_probe_amplitude.png} % B: LCLS-XPP

% ---------- GLOBAL FONTS ----------
\newcommand{\figfs}{\normalsize}
\newcommand{\scalefs}{\small\bfseries}

% ---------- DIMENSIONS ----------
\newlength{\subw}
\setlength{\subw}{5.0cm}
\newlength{\subh}
\setlength{\subh}{5.0cm}
\newlength{\subsepX}
\setlength{\subsepX}{4mm}
\newlength{\panelsepX}
\setlength{\panelsepX}{16mm}   % wider to keep clear of GT
\newlength{\panelsepY}
\setlength{\panelsepY}{11mm}   % gap between rows

\newlength{\refw}
\setlength{\refw}{5.0cm}
\newlength{\refh}
\setlength{\refh}{5.0cm}
\newlength{\refgap}
\setlength{\refgap}{10mm}

\newlength{\cbarw}
\setlength{\cbarw}{6mm}
\newlength{\cbarh}
\setlength{\cbarh}{5.0cm}
\newlength{\cbarsep}
\setlength{\cbarsep}{4mm}

% ---------- INCLUDE / PLACEHOLDER ----------
\newcommand{\IncludeOrPlaceholderWH}[3]{%
  \IfFileExists{#1}{\includegraphics[width=#2,height=#3,keepaspectratio]{#1}}{%
    \fcolorbox{black!20}{black!5}{\parbox[c][#3][c]{#2}{\centering\footnotesize Missing}}}%
}

% ---------- PANEL FRAMES ----------
\tikzset{panelframe/.style={draw=black!60, line width=0.25pt, rounded corners=0.8pt}}
\newcommand{\FramePanel}[1]{\draw[panelframe] (#1.north west) rectangle (#1.south east);}

% ---------- SCALE BAR ----------
\newcommand{\AddScaleBar}[3]{% #1=node, #2=length, #3=label
  \draw[line width=2pt, black]
    ($ (#1.south east) + (-1.4cm,0.28cm) $) --
    ($ (#1.south east) + (-1.4cm+#2,0.28cm) $);
  \draw[line width=1.2pt, white]
    ($ (#1.south east) + (-1.4cm,0.28cm) $) --
    ($ (#1.south east) + (-1.4cm+#2,0.28cm) $);
  \node[anchor=south, font=\scalefs, text=white, fill=black,
        inner sep=1.2pt, rounded corners=0.6pt]
    at ($ (#1.south east) + (-1.4cm+#2/2,0.62cm) $) {#3};
}

% ---------- COLORBARS (avoid overlap) ----------
\newcommand{\ampColorbar}{figures/out-dist-fig/amplitude_colorbar.png}
\newcommand{\phaseColorbar}{figures/out-dist-fig/phase_colorbar.png}
\newcommand{\IncludeCbarLeftOf}[2]{%
  \IfFileExists{#2}{%
    \node[anchor=north east] at ($(#1.north west) + (-\cbarsep,0)$)
      {\includegraphics[height=\cbarh]{#2}}; }{}%
}
\newcommand{\IncludeCbarRightOf}[2]{%
  \IfFileExists{#2}{%
    \node[anchor=north west] at ($(#1.north east) + (\cbarsep,0)$)
      {\includegraphics[height=\cbarh]{#2}}; }{}%
}

% ---------- Header & Legend styles ----------
\tikzset{
  headerbox/.style={
    draw=black!30, rounded corners=2pt, fill=white,
    align=center, inner xsep=6pt, inner ysep=4pt, font=\small\bfseries
  },
  legendbox/.style={
    draw=black!30, rounded corners=2pt, fill=white,
    inner xsep=6pt, inner ysep=6pt
  }
}

% Begin TikZ content
% Separate lifts for headers
\newlength{\hdrsepRowOne}
\setlength{\hdrsepRowOne}{4mm}   % distance above ROW 1
\newlength{\hdrsepGT}
\setlength{\hdrsepGT}{3mm}     % GT header proximity
\newlength{\gtlift}
\setlength{\gtlift}{30mm}      % raise GT pair

\begin{tikzpicture}[font=\figfs, every node/.style={inner sep=0, outer sep=0}, >=latex]

% Declare layers
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% =======================
% Top row (PINN)
% =======================
\node[anchor=north west] (p11a) at (0,0)
  {\IncludeOrPlaceholderWH{\pinnIDamp}{\subw}{\subh}};
\node[anchor=north west] (p11p) at ($(p11a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnIDphs}{\subw}{\subh}};
\node[fit=(p11a)(p11p), inner sep=0pt] (cell11) {};
% \AddScaleBar{p11a}{1cm}{X nm} \AddScaleBar{p11p}{1cm}{X nm}
\FramePanel{p11a} \FramePanel{p11p}

% Top row middle (PINN OOD)
\node[anchor=north west] (p12a) at ($(cell11.north east) + (\panelsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnOODamp}{\subw}{\subh}};
\node[anchor=north west] (p12p) at ($(p12a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\pinnOODphs}{\subw}{\subh}};
\node[fit=(p12a)(p12p), inner sep=0pt] (cell12) {};
% \AddScaleBar{p12a}{1cm}{X nm} \AddScaleBar{p12p}{1cm}{X nm}
\FramePanel{p12a} \FramePanel{p12p}

% =======================
% Bottom row (PtychoNN)
% =======================
\node[anchor=north west] (p21a) at ($(cell11.south west) + (0,-\panelsepY)$)
  {\IncludeOrPlaceholderWH{\baseIDamp}{\subw}{\subh}};
\node[anchor=north west] (p21p) at ($(p21a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\baseIDphs}{\subw}{\subh}};
\node[fit=(p21a)(p21p), inner sep=0pt] (cell21) {};
% \AddScaleBar{p21a}{1cm}{X nm} \AddScaleBar{p21p}{1cm}{X nm}
\FramePanel{p21a} \FramePanel{p21p}

\node[anchor=north west] (p22a) at ($(p12a.south west) + (0,-\panelsepY)$)
  {\IncludeOrPlaceholderWH{\baseOODamp}{\subw}{\subh}};
\node[anchor=north west] (p22p) at ($(p22a.north east) + (\subsepX,0)$)
  {\IncludeOrPlaceholderWH{\baseOODphs}{\subw}{\subh}};
\node[fit=(p22a)(p22p), inner sep=0pt] (cell22) {};
% \AddScaleBar{p22a}{1cm}{X nm} \AddScaleBar{p22p}{1cm}{X nm}
\FramePanel{p22a} \FramePanel{p22p}

% =======================
% Reference column (raised)
% =======================
\node[fit=(cell11)(cell21), inner sep=0pt] (bothrows) {};
\coordinate (gt-x) at ($ (cell12.north east) + (\panelsepX,0) $);
\node[anchor=west, yshift=\gtlift] (ref-amp) at ($(bothrows.center |- gt-x)$)
  {\IncludeOrPlaceholderWH{\refAmp}{\refw}{\refh}};
\node[anchor=north west] (ref-phs) at ($(ref-amp.north east) + (\refgap,0)$)
  {\IncludeOrPlaceholderWH{\refPhs}{\refw}{\refh}};
\node[fit=(ref-amp)(ref-phs), inner sep=0pt] (refcol) {};
% Scale bars removed - pixel size TBD
% \AddScaleBar{ref-amp}{1cm}{X nm} \AddScaleBar{ref-phs}{1cm}{X nm}
\FramePanel{ref-amp} \FramePanel{ref-phs}

% Colorbars (no overlap with GT)
\IncludeCbarLeftOf{ref-amp}{\ampColorbar}
\IncludeCbarRightOf{ref-phs}{\phaseColorbar}

% =======================
% Fit nodes for rows & grid
% =======================
\node[fit=(p11a)(p11p)(p12a)(p12p), inner sep=0pt] (row1grid) {};
\node[fit=(p21a)(p21p)(p22a)(p22p), inner sep=0pt] (row2grid) {};
\node[fit=(cell11)(cell12)(cell21)(cell22), inner sep=0pt] (grid) {};

% =======================
% Gray separator line above ROW 2 (and below GT) - REMOVED
% =======================

% =======================
% Boxed headers placed correctly
% =======================
\node[headerbox, anchor=south east] (hdr1) at ($(grid.north east)+(0,\hdrsepRowOne)$) {In-Distribution (Train B $\rightarrow$ Test B)};
\node[headerbox, anchor=south west] (hdr2) at ($(grid.north west)+(0,\hdrsepRowOne)$) {Out-of-Distribution (Train A $\rightarrow$ Test B)};
\node[headerbox] (hdr3) at ($(refcol.north)+(0,\hdrsepGT)$)        {Reference (ePIE)};

% =======================
% Row labels (included in bbox)
% =======================
\node[anchor=east, font=\bfseries\figfs, text=pinncol] (rowPINN)
  at ($ (cell11.west) + (-5mm,0) $) {PINN};
\node[anchor=east, font=\bfseries\figfs, text=basecol] (rowBASE)
  at ($ (cell21.west) + (-5mm,0) $) {PtychoNN};

% =======================
% Top-left legend (foreground; never hidden)
% =======================
\begin{pgfonlayer}{foreground}
  % Create invisible matrix first to get positions
  \matrix (L) [matrix of nodes,
               row sep=5mm, column sep=4mm,
               nodes={anchor=west, font=\small},
               anchor=south west] at ($ (grid.north west) + (0, \hdrsepRowOne + 12mm) $)
  {
    \node[font=\small\bfseries] {A}; & APS-2-ID; & \node (Aimg) {}; \\
    \node[font=\small\bfseries] {B}; & LCLS-XPP; & \node (Bimg) {}; \\
  };
  % Legend background box fitted around the matrix & images (draw first)
  \node[legendbox, fit=(L) (Aimg) (Bimg)] (legend) {};
  % Now draw content on top
  \matrix (L) [matrix of nodes,
               row sep=5mm, column sep=4mm,
               nodes={anchor=west, font=\small},
               anchor=south west] at ($ (grid.north west) + (0, \hdrsepRowOne + 12mm) $)
  {
    \node[font=\small\bfseries] {A}; & APS-2-ID; & \node (Aimg) {}; \\
    \node[font=\small\bfseries] {B}; & LCLS-XPP; & \node (Bimg) {}; \\
  };
  % Probe thumbnails in legend
  \def\lgr{0.35cm}
  \begin{scope}
    \clip (Aimg.center) circle (\lgr);
    \node at (Aimg.center) {\IncludeOrPlaceholderWH{\probeA}{0.9cm}{0.9cm}};
  \end{scope}
  \draw[black!70, line width=0.4pt] (Aimg.center) circle (\lgr);
  \begin{scope}
    \clip (Bimg.center) circle (\lgr);
    \node at (Bimg.center) {\IncludeOrPlaceholderWH{\probeB}{0.9cm}{0.9cm}};
  \end{scope}
  \draw[black!70, line width=0.4pt] (Bimg.center) circle (\lgr);
\end{pgfonlayer}

% =======================
% Tight bounding box (include legend!)
% =======================
\node[fit=(legend)(hdr1)(hdr2)(hdr3)(rowPINN)(rowBASE)(refcol)(cell11)(cell12)(cell21)(cell22), inner sep=0pt] (ALL) {};
\begin{pgfonlayer}{background}
  \path[use as bounding box] (ALL.north west) rectangle (ALL.south east);
\end{pgfonlayer}

\end{tikzpicture}
% End TikZ content
  }% End resizebox
  \caption{Comparison of methods for in-distribution reconstruction (train APS, test APS) and out-of-distribution transfer (train APS, test LCLS). The reference column shows an ePIE reconstruction of the LCLS data.}
  \label{fig:fivepanel}
\end{figure}
% The baseline U-Net model exhibited catastrophic failure on out-of-distribution data, producing near-constant outputs (σ_amplitude < 0.01) compared to in-distribution performance (σ_amplitude ≈ 0.03).
%  This mode collapse occurs despite input rescaling, indicating the model has overfit to the specific speckle statistics of the training distribution rather than learning transferable features. The
%  baseline model's purely supervised training lacks the inductive bias provided by physics constraints, making it vulnerable to distribution shift even when imaging the same sample type under different
%  experimental conditions.

\subsection{Computational Performance}

PtychoPINN processes approximately 4.6k diffraction patterns per second on an NVIDIA RTX 3090, excluding stitching/reassembly time. 

\section{Discussion}

\subsection*{Physics-constrained flexibility and single-shot}
In classical ptychography, translational overlap provides real-space redundancy for phase retrieval. In our framework, the group size ($C_g$) explicitly controls overlap, so redundancy can come either from inter-measurement overlap or from structured probe phase within a single exposure. This flexibility is particularly valuable in the Fresnel (curved-probe) regime, where we observe stable single-shot reconstructions (Fig.~\ref{fig:recon_2x2}). Conventional solvers typically require overlap by construction, whereas our formulation supports reconstruction with or without overlap when the probe provides sufficient phase diversity.

\subsection*{Dose efficiency}
The Poisson NLL objective matches photon-counting statistics, providing correct likelihood scoring at low intensities where MAE is biased. Because high-$q$ diffraction signal is weak and noise-dominated, an objective that properly weights low-count pixels improves dose efficiency (Fig.~\ref{fig:dose}). This is consistent with qualitative comparisons of low-dose reconstructions (Fig.~\ref{fig:lowcounts}).

\subsection*{Generalization and data efficiency}
On the Siemens-star spatial holdout, the supervised baseline shows a clear train--test gap, while the physics-informed model maintains comparable resolution across held-out positions (Figs.~\ref{fig:smalldat}, \ref{fig:ssim}). Under a distribution shift (APS$\!\to\!$LCLS), the baseline degrades markedly, whereas the physics-informed model preserves edge structure despite phase distortions (Fig.~\ref{fig:fivepanel}). The SSIM curves further indicate that the physics-constrained model reaches a given fidelity with far fewer training samples (Fig.~\ref{fig:ssim}). Together, these results point to a strong inductive bias from the diffraction forward model that improves both data efficiency and transfer robustness.

\subsection*{Implications for modern light sources}
These capabilities relax overlap requirements and reduce the burden of labeled training data, enabling more flexible experimental design and faster feedback in high-rate CDI workflows. Single-shot Fresnel CDI and improved dose efficiency are particularly relevant for dynamic or radiation-sensitive samples.

\subsection*{Limitations and extensions}
We assume a fixed, pre-estimated probe during both training and inference; joint probe retrieval within the self-supervised loop is a natural extension. Probe drift and position errors are not modeled explicitly and could be addressed by stochastic position models or differentiable refinement. Extending the approach to Bragg CDI and reflection geometries would primarily require forward-model and sampling adaptations.

\section{Conclusion}
Coupling a Poisson photon-counting likelihood with a differentiable forward model yields dose efficiency, robustness to sparse or irregular acquisition, and support for single-shot Fresnel CDI. These properties broaden the practical operating regime of coherent imaging at modern light sources.



% [Internal notes removed per revision plan A2]

\newpage
\section*{Appendix A: Key Configuration Parameters}
These parameters control critical aspects of the reconstruction process and should be tuned based on experimental conditions and computational constraints.
\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Model parameters and their default values}
\label{tab:config_params}
\begin{tabular}{lcl}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Description} \\
\hline
\texttt{N} & 64 & Patch dimension (pixels) \\
\texttt{C\_g} & 4 & Number of patterns per group \\
\texttt{K} & 7 & Nearest neighbors for grouping \\
\texttt{nsamples} & 1 & Random samplings per scan point \\
\texttt{pad\_object} & True & Enable adaptive boundary learning \\
\texttt{probe.mask} & True & Apply circular probe support \\
\texttt{gaussian\_smoothing\_sigma} & 0.0 & Probe boundary smoothing \\
\texttt{intensity\_scale.trainable} & False & Learnable intensity scaling \\
\texttt{n\_filters\_scale} & 1 & Network width multiplier \\
\texttt{amp\_activation} & sigmoid & Amplitude decoder activation \\
\texttt{offset} & 4 & Scan step size (pixels) \\
$d$ & 3-5 & Encoder depth (resolution-dependent) \\
$C$ & 132 & Total decoder channels (before split) \\
$C_{\text{latent}}$ & 128 & Latent channels at bottleneck \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Mathematical symbols and conceptual descriptions}
\label{tab:symbols}
\begin{tabular}{lcl}
\hline
\textbf{Symbol} & \textbf{Type / Structure} & \textbf{Description} \\
\hline
$x'$ & Set of $C_g$ real images & Raw diffraction patterns for one sample \\
$x$ & Set of $C_g$ real images & Normalized diffraction patterns for one sample \\
$\vec{r}_k$ & 2D Position Vector & Absolute scan position for the $k$-th image within a sample \\
$\vec{r}_{\text{global}}^i$ & 2D Position Vector & Centroid of a solution region (group of scans) \\
$\vec{r}_{\text{relative},k}$ & 2D Offset Vector & Relative scan offset within a solution region \\
$e^{\alpha_{\log}}$ & Scalar (trainable) & Internal log-intensity scale parameter \\
$N_{\text{photons}}$ & Scalar & Target average total photons per diffraction pattern \\
$P(\vec{r})$ & $N \times N$ complex array & Effective probe function \\
$O_k$ & $N \times N$ complex array & $k$-th object patch decoded by the network $G$ \\
$O_{\text{region}}$ & $M \times M$ complex array & Merged object representation for a solution region \\
$O'_{\text{k}}$ & $N \times N$ complex array & Object patch extracted from $O_{\text{region}}$ for forward model \\
$\Psi_k$ & $N \times N$ complex array & Predicted complex wavefield at the detector \\
$\hat{A}_k$ & $N \times N$ real array & Predicted final diffraction amplitude for one patch \\
$\lambda_{ijk}$ & Scalar & Poisson rate parameter for a single pixel \\
\hline
\multicolumn{3}{l}{\footnotesize $N$: patch dimension, $C_g$: patches per group, $M$: merged region size}
\end{tabular}
\end{table}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/Figure1.pdf}
%     \caption{Estimates of raw data generation rate for light sources over time }
%     \label{fig:Figure1}
% \end{figure}

\bibliographystyle{plain}
\bibliography{references}


\end{document}
